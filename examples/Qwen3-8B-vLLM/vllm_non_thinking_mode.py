"""
Qwen3-8B vLLM éæ€è€ƒæ¨¡å¼æ¨ç†ç¤ºä¾‹

æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨ vLLM è¿›è¡Œ Qwen3-8B æ¨¡å‹çš„éæ€è€ƒæ¨¡å¼æ¨ç†
éæ€è€ƒæ¨¡å¼ç›´æ¥ç»™å‡ºç­”æ¡ˆï¼Œä¸æ˜¾ç¤ºæ¨ç†è¿‡ç¨‹
"""

import os
from pathlib import Path
from vllm import LLM, SamplingParams
from transformers import AutoTokenizer


def get_model_path() -> str:
    """è·å–æ¨¡å‹è·¯å¾„"""
    # ä¼˜å…ˆæ£€æŸ¥ AutoDL ç¯å¢ƒ
    autodl_path = "/root/autodl-tmp/Qwen/Qwen3-8B"
    if os.path.exists(autodl_path):
        return autodl_path
    
    # æ£€æŸ¥æœ¬åœ° models ç›®å½•
    local_path = "./models/Qwen/Qwen3-8B"
    if os.path.exists(local_path):
        return local_path
    
    # å¦‚æœéƒ½ä¸å­˜åœ¨ï¼Œè¿”å›é»˜è®¤è·¯å¾„å¹¶æç¤ºç”¨æˆ·
    print("âš ï¸  æœªæ‰¾åˆ°æ¨¡å‹æ–‡ä»¶ï¼Œè¯·å…ˆè¿è¡Œ model_download.py ä¸‹è½½æ¨¡å‹")
    return local_path


def create_non_thinking_prompt(user_input: str) -> str:
    """
    åˆ›å»ºéæ€è€ƒæ¨¡å¼çš„æç¤ºè¯
    
    Args:
        user_input: ç”¨æˆ·è¾“å…¥
    
    Returns:
        str: æ ¼å¼åŒ–åçš„æç¤ºè¯
    """
    messages = [
        {"role": "user", "content": user_input}
    ]
    
    # è·å–æ¨¡å‹è·¯å¾„å¹¶åŠ è½½åˆ†è¯å™¨
    model_path = get_model_path()
    tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)
    
    # åº”ç”¨èŠå¤©æ¨¡æ¿ï¼Œç¦ç”¨æ€è€ƒæ¨¡å¼
    text = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True,
        enable_thinking=False  # ç¦ç”¨æ€è€ƒæ¨¡å¼
    )
    
    return text


def get_completion_non_thinking(
    prompts: list,
    model_path: str,
    temperature: float = 0.7,
    top_p: float = 0.8,
    top_k: int = 20,
    min_p: float = 0,
    max_tokens: int = 4096,
    max_model_len: int = 8192
):
    """
    ä½¿ç”¨éæ€è€ƒæ¨¡å¼è¿›è¡Œæ–‡æœ¬ç”Ÿæˆ
    
    Args:
        prompts: æç¤ºè¯åˆ—è¡¨
        model_path: æ¨¡å‹è·¯å¾„
        temperature: æ¸©åº¦å‚æ•°ï¼Œæ§åˆ¶ç”Ÿæˆæ–‡æœ¬çš„éšæœºæ€§
        top_p: æ ¸å¿ƒé‡‡æ ·æ¦‚ç‡
        top_k: å€™é€‰è¯æ•°é‡é™åˆ¶
        min_p: æœ€å°æ¦‚ç‡é˜ˆå€¼
        max_tokens: æœ€å¤§ç”Ÿæˆé•¿åº¦
        max_model_len: æ¨¡å‹æœ€å¤§é•¿åº¦
    
    Returns:
        ç”Ÿæˆç»“æœåˆ—è¡¨
    """
    # è®¾ç½®åœæ­¢è¯ ID
    stop_token_ids = [151645, 151643]
    
    # åˆ›å»ºé‡‡æ ·å‚æ•° - éæ€è€ƒæ¨¡å¼æ¨èå‚æ•°
    sampling_params = SamplingParams(
        temperature=temperature,
        top_p=top_p,
        top_k=top_k,
        min_p=min_p,
        max_tokens=max_tokens,
        stop_token_ids=stop_token_ids
    )
    
    # åˆå§‹åŒ– vLLM æ¨ç†å¼•æ“
    llm = LLM(
        model=model_path,
        max_model_len=max_model_len,
        trust_remote_code=True
    )
    
    # ç”Ÿæˆæ–‡æœ¬
    outputs = llm.generate(prompts, sampling_params)
    return outputs


def clean_response(response_text: str) -> str:
    """
    æ¸…ç†å“åº”æ–‡æœ¬ï¼Œç§»é™¤å¯èƒ½çš„æ€è€ƒæ ‡ç­¾
    
    Args:
        response_text: åŸå§‹å“åº”æ–‡æœ¬
    
    Returns:
        str: æ¸…ç†åçš„å“åº”æ–‡æœ¬
    """
    # ç§»é™¤å¯èƒ½å­˜åœ¨çš„ç©ºæ€è€ƒæ ‡ç­¾
    if "<think>" in response_text and "</think>" in response_text:
        start_idx = response_text.find("</think>") + 8
        return response_text[start_idx:].strip()
    
    return response_text.strip()


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¤– Qwen3-8B vLLM éæ€è€ƒæ¨¡å¼æ¨ç†ç¤ºä¾‹")
    print("=" * 50)
    
    # è®¾ç½®ç¯å¢ƒå˜é‡
    os.environ['VLLM_USE_MODELSCOPE'] = 'True'
    
    # è·å–æ¨¡å‹è·¯å¾„
    model_path = get_model_path()
    print(f"ğŸ“ æ¨¡å‹è·¯å¾„: {model_path}")
    
    # æµ‹è¯•é—®é¢˜åˆ—è¡¨
    test_questions = [
        "ä½ æ˜¯è°ï¼Ÿ",
        "ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ",
        "è¯·ç”¨ä¸€å¥è¯ä»‹ç»äººå·¥æ™ºèƒ½ã€‚",
        "å†™ä¸€ä¸ªç®€å•çš„ Python å‡½æ•°æ¥è®¡ç®—æ–æ³¢é‚£å¥‘æ•°åˆ—ã€‚",
        "æ¨èå‡ æœ¬å…³äºæœºå™¨å­¦ä¹ çš„ä¹¦ç±ã€‚"
    ]
    
    for i, question in enumerate(test_questions, 1):
        print(f"\nğŸ” æµ‹è¯•é—®é¢˜ {i}: {question}")
        print("-" * 40)
        
        try:
            # åˆ›å»ºéæ€è€ƒæ¨¡å¼æç¤ºè¯
            prompt = create_non_thinking_prompt(question)
            
            # è¿›è¡Œæ¨ç†
            print("â³ æ­£åœ¨ç”Ÿæˆå›ç­”...")
            outputs = get_completion_non_thinking([prompt], model_path)
            
            # è¾“å‡ºç»“æœ
            for output in outputs:
                generated_text = output.outputs[0].text
                clean_text = clean_response(generated_text)
                
                print(f"\nâœ… å›ç­”:")
                print(clean_text)
                
        except Exception as e:
            print(f"âŒ æ¨ç†è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
    
    print("\n" + "=" * 50)
    print("ğŸ‰ éæ€è€ƒæ¨¡å¼æ¨ç†ç¤ºä¾‹å®Œæˆï¼")
    
    # äº¤äº’å¼å¯¹è¯
    print("\nğŸ’¬ è¿›å…¥äº¤äº’å¼å¯¹è¯æ¨¡å¼ (è¾“å…¥ 'quit' é€€å‡º):")
    print("-" * 40)
    
    while True:
        try:
            user_input = input("\nğŸ‘¤ ç”¨æˆ·: ").strip()
            
            if user_input.lower() in ['quit', 'exit', 'é€€å‡º', 'q']:
                print("ğŸ‘‹ å†è§ï¼")
                break
            
            if not user_input:
                continue
            
            # åˆ›å»ºæç¤ºè¯å¹¶æ¨ç†
            prompt = create_non_thinking_prompt(user_input)
            outputs = get_completion_non_thinking([prompt], model_path)
            
            # è¾“å‡ºå›ç­”
            for output in outputs:
                generated_text = output.outputs[0].text
                clean_text = clean_response(generated_text)
                print(f"\nğŸ¤– åŠ©æ‰‹: {clean_text}")
                
        except KeyboardInterrupt:
            print("\nğŸ‘‹ å†è§ï¼")
            break
        except Exception as e:
            print(f"âŒ å¯¹è¯è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")


if __name__ == "__main__":
    main()
