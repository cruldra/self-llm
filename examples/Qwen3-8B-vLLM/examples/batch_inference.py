"""
æ‰¹é‡æ¨ç†ç¤ºä¾‹

æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨ vLLM è¿›è¡Œé«˜æ•ˆçš„æ‰¹é‡æ¨ç†
"""

import os
import time
import json
from pathlib import Path
from typing import List, Dict, Any
from vllm import LLM, SamplingParams
from transformers import AutoTokenizer


def get_model_path() -> str:
    """è·å–æ¨¡å‹è·¯å¾„"""
    # ä¼˜å…ˆæ£€æŸ¥ AutoDL ç¯å¢ƒ
    autodl_path = "/root/autodl-tmp/Qwen/Qwen3-8B"
    if os.path.exists(autodl_path):
        return autodl_path
    
    # æ£€æŸ¥æœ¬åœ° models ç›®å½•
    local_path = "../models/Qwen/Qwen3-8B"
    if os.path.exists(local_path):
        return local_path
    
    # æ£€æŸ¥ä¸Šçº§ç›®å½•çš„ models
    parent_path = "./models/Qwen/Qwen3-8B"
    if os.path.exists(parent_path):
        return parent_path
    
    print("âš ï¸  æœªæ‰¾åˆ°æ¨¡å‹æ–‡ä»¶ï¼Œè¯·å…ˆè¿è¡Œ model_download.py ä¸‹è½½æ¨¡å‹")
    return local_path


class BatchInferenceEngine:
    """æ‰¹é‡æ¨ç†å¼•æ“"""
    
    def __init__(self, model_path: str = None, max_model_len: int = 8192):
        """
        åˆå§‹åŒ–æ‰¹é‡æ¨ç†å¼•æ“
        
        Args:
            model_path: æ¨¡å‹è·¯å¾„
            max_model_len: æ¨¡å‹æœ€å¤§é•¿åº¦
        """
        self.model_path = model_path or get_model_path()
        self.max_model_len = max_model_len
        
        # è®¾ç½®ç¯å¢ƒå˜é‡
        os.environ['VLLM_USE_MODELSCOPE'] = 'True'
        
        # åŠ è½½åˆ†è¯å™¨
        print(f"ğŸ“ åŠ è½½åˆ†è¯å™¨: {self.model_path}")
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.model_path, 
            use_fast=False
        )
        
        # åˆå§‹åŒ– vLLM å¼•æ“
        print(f"ğŸš€ åˆå§‹åŒ– vLLM å¼•æ“...")
        self.llm = LLM(
            model=self.model_path,
            max_model_len=self.max_model_len,
            trust_remote_code=True
        )
        
        print(f"âœ… æ‰¹é‡æ¨ç†å¼•æ“åˆå§‹åŒ–å®Œæˆ")
    
    def prepare_prompts(
        self, 
        inputs: List[Dict[str, Any]], 
        enable_thinking: bool = False
    ) -> List[str]:
        """
        å‡†å¤‡æ‰¹é‡æ¨ç†çš„æç¤ºè¯
        
        Args:
            inputs: è¾“å…¥æ•°æ®åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å« messages æˆ– prompt
            enable_thinking: æ˜¯å¦å¯ç”¨æ€è€ƒæ¨¡å¼
        
        Returns:
            List[str]: æ ¼å¼åŒ–åçš„æç¤ºè¯åˆ—è¡¨
        """
        prompts = []
        
        for input_data in inputs:
            if "messages" in input_data:
                # å¯¹è¯æ ¼å¼
                prompt = self.tokenizer.apply_chat_template(
                    input_data["messages"],
                    tokenize=False,
                    add_generation_prompt=True,
                    enable_thinking=enable_thinking
                )
            elif "prompt" in input_data:
                # ç›´æ¥æç¤ºè¯æ ¼å¼
                prompt = input_data["prompt"]
            else:
                raise ValueError("è¾“å…¥æ•°æ®å¿…é¡»åŒ…å« 'messages' æˆ– 'prompt' å­—æ®µ")
            
            prompts.append(prompt)
        
        return prompts
    
    def batch_generate(
        self,
        inputs: List[Dict[str, Any]],
        enable_thinking: bool = False,
        temperature: float = 0.7,
        top_p: float = 0.8,
        top_k: int = 20,
        max_tokens: int = 2048
    ) -> List[Dict[str, Any]]:
        """
        æ‰¹é‡ç”Ÿæˆæ–‡æœ¬
        
        Args:
            inputs: è¾“å…¥æ•°æ®åˆ—è¡¨
            enable_thinking: æ˜¯å¦å¯ç”¨æ€è€ƒæ¨¡å¼
            temperature: æ¸©åº¦å‚æ•°
            top_p: æ ¸å¿ƒé‡‡æ ·æ¦‚ç‡
            top_k: å€™é€‰è¯æ•°é‡é™åˆ¶
            max_tokens: æœ€å¤§ç”Ÿæˆé•¿åº¦
        
        Returns:
            List[Dict[str, Any]]: ç”Ÿæˆç»“æœåˆ—è¡¨
        """
        # å‡†å¤‡æç¤ºè¯
        prompts = self.prepare_prompts(inputs, enable_thinking)
        
        # è®¾ç½®é‡‡æ ·å‚æ•°
        if enable_thinking:
            # æ€è€ƒæ¨¡å¼æ¨èå‚æ•°
            sampling_params = SamplingParams(
                temperature=0.6,
                top_p=0.95,
                top_k=20,
                min_p=0,
                max_tokens=max_tokens,
                stop_token_ids=[151645, 151643]
            )
        else:
            # æ™®é€šæ¨¡å¼å‚æ•°
            sampling_params = SamplingParams(
                temperature=temperature,
                top_p=top_p,
                top_k=top_k,
                max_tokens=max_tokens,
                stop_token_ids=[151645, 151643]
            )
        
        # è®°å½•å¼€å§‹æ—¶é—´
        start_time = time.time()
        
        # æ‰¹é‡ç”Ÿæˆ
        print(f"â³ å¼€å§‹æ‰¹é‡æ¨ç† ({len(prompts)} ä¸ªè¯·æ±‚)...")
        outputs = self.llm.generate(prompts, sampling_params)
        
        # è®°å½•ç»“æŸæ—¶é—´
        end_time = time.time()
        duration = end_time - start_time
        
        print(f"âœ… æ‰¹é‡æ¨ç†å®Œæˆï¼Œè€—æ—¶: {duration:.2f} ç§’")
        print(f"ğŸ“Š å¹³å‡æ¯ä¸ªè¯·æ±‚: {duration/len(prompts):.2f} ç§’")
        
        # å¤„ç†ç»“æœ
        results = []
        for i, output in enumerate(outputs):
            generated_text = output.outputs[0].text
            
            # è§£ææ€è€ƒæ¨¡å¼å“åº”
            thinking_content = ""
            final_answer = generated_text
            
            if enable_thinking and "<think>" in generated_text and "</think>" in generated_text:
                start_idx = generated_text.find("<think>") + 7
                end_idx = generated_text.find("</think>")
                thinking_content = generated_text[start_idx:end_idx].strip()
                final_answer = generated_text[end_idx + 8:].strip()
            elif "<think>" in generated_text and "</think>" in generated_text:
                # æ¸…ç†ç©ºæ€è€ƒæ ‡ç­¾
                start_idx = generated_text.find("</think>") + 8
                final_answer = generated_text[start_idx:].strip()
            
            result = {
                "index": i,
                "input": inputs[i],
                "output": final_answer,
                "thinking": thinking_content,
                "raw_output": generated_text,
                "finish_reason": output.outputs[0].finish_reason
            }
            results.append(result)
        
        return results


def demo_qa_batch():
    """é—®ç­”æ‰¹é‡æ¨ç†æ¼”ç¤º"""
    print("\nğŸ“š é—®ç­”æ‰¹é‡æ¨ç†æ¼”ç¤º")
    print("=" * 40)
    
    # åˆå§‹åŒ–æ¨ç†å¼•æ“
    engine = BatchInferenceEngine()
    
    # å‡†å¤‡é—®ç­”æ•°æ®
    qa_inputs = [
        {
            "id": "qa_1",
            "messages": [
                {"role": "user", "content": "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ"}
            ]
        },
        {
            "id": "qa_2", 
            "messages": [
                {"role": "user", "content": "æ·±åº¦å­¦ä¹ å’Œæœºå™¨å­¦ä¹ æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ"}
            ]
        },
        {
            "id": "qa_3",
            "messages": [
                {"role": "user", "content": "ä»€ä¹ˆæ˜¯ç¥ç»ç½‘ç»œï¼Ÿ"}
            ]
        },
        {
            "id": "qa_4",
            "messages": [
                {"role": "user", "content": "è§£é‡Šä¸€ä¸‹ä»€ä¹ˆæ˜¯è¿‡æ‹Ÿåˆï¼Ÿ"}
            ]
        },
        {
            "id": "qa_5",
            "messages": [
                {"role": "user", "content": "ä»€ä¹ˆæ˜¯æ¢¯åº¦ä¸‹é™ï¼Ÿ"}
            ]
        }
    ]
    
    # æ‰¹é‡æ¨ç†
    results = engine.batch_generate(qa_inputs, enable_thinking=False)
    
    # æ˜¾ç¤ºç»“æœ
    for result in results:
        print(f"\nğŸ“ é—®é¢˜ {result['index'] + 1}: {result['input']['messages'][0]['content']}")
        print(f"ğŸ¤– å›ç­”: {result['output'][:200]}...")
        print(f"ğŸ ç»“æŸåŸå› : {result['finish_reason']}")


def demo_code_generation_batch():
    """ä»£ç ç”Ÿæˆæ‰¹é‡æ¨ç†æ¼”ç¤º"""
    print("\nğŸ’» ä»£ç ç”Ÿæˆæ‰¹é‡æ¨ç†æ¼”ç¤º")
    print("=" * 40)
    
    # åˆå§‹åŒ–æ¨ç†å¼•æ“
    engine = BatchInferenceEngine()
    
    # å‡†å¤‡ä»£ç ç”Ÿæˆä»»åŠ¡
    code_inputs = [
        {
            "task": "æ’åºç®—æ³•",
            "messages": [
                {"role": "user", "content": "å†™ä¸€ä¸ª Python å‡½æ•°å®ç°å¿«é€Ÿæ’åºç®—æ³•"}
            ]
        },
        {
            "task": "æ•°æ®ç»“æ„",
            "messages": [
                {"role": "user", "content": "å®ç°ä¸€ä¸ªç®€å•çš„é“¾è¡¨ç±»"}
            ]
        },
        {
            "task": "ç®—æ³•é¢˜",
            "messages": [
                {"role": "user", "content": "å†™ä¸€ä¸ªå‡½æ•°åˆ¤æ–­ä¸€ä¸ªå­—ç¬¦ä¸²æ˜¯å¦æ˜¯å›æ–‡"}
            ]
        },
        {
            "task": "å·¥å…·å‡½æ•°",
            "messages": [
                {"role": "user", "content": "å†™ä¸€ä¸ªå‡½æ•°è®¡ç®—ä¸¤ä¸ªæ•°çš„æœ€å¤§å…¬çº¦æ•°"}
            ]
        }
    ]
    
    # æ‰¹é‡æ¨ç†
    results = engine.batch_generate(
        code_inputs, 
        enable_thinking=False,
        temperature=0.2,  # ä»£ç ç”Ÿæˆä½¿ç”¨è¾ƒä½æ¸©åº¦
        max_tokens=1024
    )
    
    # æ˜¾ç¤ºç»“æœ
    for result in results:
        print(f"\nğŸ’» ä»»åŠ¡: {result['input']['task']}")
        print(f"ğŸ“ è¦æ±‚: {result['input']['messages'][0]['content']}")
        print(f"ğŸ”§ ç”Ÿæˆä»£ç :")
        print(result['output'][:500] + "..." if len(result['output']) > 500 else result['output'])


def demo_thinking_batch():
    """æ€è€ƒæ¨¡å¼æ‰¹é‡æ¨ç†æ¼”ç¤º"""
    print("\nğŸ§  æ€è€ƒæ¨¡å¼æ‰¹é‡æ¨ç†æ¼”ç¤º")
    print("=" * 40)
    
    # åˆå§‹åŒ–æ¨ç†å¼•æ“
    engine = BatchInferenceEngine()
    
    # å‡†å¤‡éœ€è¦æ¨ç†çš„é—®é¢˜
    thinking_inputs = [
        {
            "type": "æ•°å­¦",
            "messages": [
                {"role": "user", "content": "è®¡ç®— 7 çš„é˜¶ä¹˜æ˜¯å¤šå°‘ï¼Ÿ"}
            ]
        },
        {
            "type": "é€»è¾‘",
            "messages": [
                {"role": "user", "content": "å¦‚æœæ‰€æœ‰çš„çŒ«éƒ½æ˜¯åŠ¨ç‰©ï¼Œè€Œå°èŠ±æ˜¯ä¸€åªçŒ«ï¼Œé‚£ä¹ˆå°èŠ±æ˜¯åŠ¨ç‰©å—ï¼Ÿ"}
            ]
        },
        {
            "type": "åˆ†æ",
            "messages": [
                {"role": "user", "content": "ä¸ºä»€ä¹ˆæ·±åº¦å­¦ä¹ åœ¨å›¾åƒè¯†åˆ«æ–¹é¢å¦‚æ­¤æˆåŠŸï¼Ÿ"}
            ]
        }
    ]
    
    # æ‰¹é‡æ¨ç†ï¼ˆå¯ç”¨æ€è€ƒæ¨¡å¼ï¼‰
    results = engine.batch_generate(
        thinking_inputs, 
        enable_thinking=True,
        max_tokens=2048
    )
    
    # æ˜¾ç¤ºç»“æœ
    for result in results:
        print(f"\nğŸ” ç±»å‹: {result['input']['type']}")
        print(f"ğŸ“ é—®é¢˜: {result['input']['messages'][0]['content']}")
        
        if result['thinking']:
            print(f"ğŸ’­ æ€è€ƒè¿‡ç¨‹: {result['thinking'][:300]}...")
            print("-" * 30)
        
        print(f"âœ… æœ€ç»ˆç­”æ¡ˆ: {result['output']}")


def demo_performance_comparison():
    """æ€§èƒ½å¯¹æ¯”æ¼”ç¤º"""
    print("\nâš¡ æ€§èƒ½å¯¹æ¯”æ¼”ç¤º")
    print("=" * 40)
    
    # åˆå§‹åŒ–æ¨ç†å¼•æ“
    engine = BatchInferenceEngine()
    
    # å‡†å¤‡æµ‹è¯•æ•°æ®
    test_inputs = [
        {
            "messages": [
                {"role": "user", "content": f"è¯·ç®€å•ä»‹ç»ä¸€ä¸‹äººå·¥æ™ºèƒ½çš„ç¬¬ {i+1} ä¸ªåº”ç”¨é¢†åŸŸã€‚"}
            ]
        }
        for i in range(10)
    ]
    
    print(f"ğŸ“Š æµ‹è¯•æ•°æ®é‡: {len(test_inputs)} ä¸ªè¯·æ±‚")
    
    # æ‰¹é‡æ¨ç†
    print("\nğŸš€ æ‰¹é‡æ¨ç†:")
    start_time = time.time()
    batch_results = engine.batch_generate(
        test_inputs,
        enable_thinking=False,
        temperature=0.7,
        max_tokens=512
    )
    batch_time = time.time() - start_time
    
    print(f"â±ï¸  æ‰¹é‡æ¨ç†æ€»æ—¶é—´: {batch_time:.2f} ç§’")
    print(f"ğŸ“ˆ å¹³å‡æ¯ä¸ªè¯·æ±‚: {batch_time/len(test_inputs):.2f} ç§’")
    print(f"ğŸ”¥ ååé‡: {len(test_inputs)/batch_time:.2f} è¯·æ±‚/ç§’")
    
    # æ˜¾ç¤ºéƒ¨åˆ†ç»“æœ
    print(f"\nğŸ“‹ éƒ¨åˆ†ç»“æœé¢„è§ˆ:")
    for i, result in enumerate(batch_results[:3]):
        print(f"  {i+1}. {result['output'][:100]}...")


def save_results_to_file(results: List[Dict[str, Any]], filename: str):
    """ä¿å­˜ç»“æœåˆ°æ–‡ä»¶"""
    output_dir = Path("batch_results")
    output_dir.mkdir(exist_ok=True)
    
    output_file = output_dir / filename
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    print(f"ğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {output_file}")


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ Qwen3-8B æ‰¹é‡æ¨ç†ç¤ºä¾‹")
    print("=" * 50)
    
    try:
        # è¿è¡Œå„ç§æ‰¹é‡æ¨ç†æ¼”ç¤º
        demo_qa_batch()
        demo_code_generation_batch()
        demo_thinking_batch()
        demo_performance_comparison()
        
    except Exception as e:
        print(f"âŒ ç¨‹åºè¿è¡Œå‡ºé”™: {e}")
    
    print("\nğŸ‰ æ‰¹é‡æ¨ç†ç¤ºä¾‹å®Œæˆï¼")


if __name__ == "__main__":
    main()
