"""
åŸºç¡€å¯¹è¯ç¤ºä¾‹

æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨ vLLM è¿›è¡ŒåŸºç¡€çš„å¯¹è¯äº¤äº’
"""

import os
from pathlib import Path
from vllm import LLM, SamplingParams
from transformers import AutoTokenizer


def get_model_path() -> str:
    """è·å–æ¨¡å‹è·¯å¾„"""
    # ä¼˜å…ˆæ£€æŸ¥ AutoDL ç¯å¢ƒ
    autodl_path = "/root/autodl-tmp/Qwen/Qwen3-8B"
    if os.path.exists(autodl_path):
        return autodl_path
    
    # æ£€æŸ¥æœ¬åœ° models ç›®å½•
    local_path = "../models/Qwen/Qwen3-8B"
    if os.path.exists(local_path):
        return local_path
    
    # æ£€æŸ¥ä¸Šçº§ç›®å½•çš„ models
    parent_path = "./models/Qwen/Qwen3-8B"
    if os.path.exists(parent_path):
        return parent_path
    
    print("âš ï¸  æœªæ‰¾åˆ°æ¨¡å‹æ–‡ä»¶ï¼Œè¯·å…ˆè¿è¡Œ model_download.py ä¸‹è½½æ¨¡å‹")
    return local_path


class QwenChatBot:
    """Qwen èŠå¤©æœºå™¨äººç±»"""
    
    def __init__(self, model_path: str = None, enable_thinking: bool = False):
        """
        åˆå§‹åŒ–èŠå¤©æœºå™¨äºº
        
        Args:
            model_path: æ¨¡å‹è·¯å¾„
            enable_thinking: æ˜¯å¦å¯ç”¨æ€è€ƒæ¨¡å¼
        """
        self.model_path = model_path or get_model_path()
        self.enable_thinking = enable_thinking
        
        # è®¾ç½®ç¯å¢ƒå˜é‡
        os.environ['VLLM_USE_MODELSCOPE'] = 'True'
        
        # åŠ è½½åˆ†è¯å™¨
        print(f"ğŸ“ åŠ è½½åˆ†è¯å™¨: {self.model_path}")
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.model_path, 
            use_fast=False
        )
        
        # åˆå§‹åŒ– vLLM å¼•æ“
        print(f"ğŸš€ åˆå§‹åŒ– vLLM å¼•æ“...")
        self.llm = LLM(
            model=self.model_path,
            max_model_len=8192,
            trust_remote_code=True
        )
        
        # è®¾ç½®é‡‡æ ·å‚æ•°
        if enable_thinking:
            # æ€è€ƒæ¨¡å¼æ¨èå‚æ•°
            self.sampling_params = SamplingParams(
                temperature=0.6,
                top_p=0.95,
                top_k=20,
                min_p=0,
                max_tokens=4096,
                stop_token_ids=[151645, 151643]
            )
        else:
            # éæ€è€ƒæ¨¡å¼æ¨èå‚æ•°
            self.sampling_params = SamplingParams(
                temperature=0.7,
                top_p=0.8,
                top_k=20,
                min_p=0,
                max_tokens=4096,
                stop_token_ids=[151645, 151643]
            )
        
        print(f"âœ… èŠå¤©æœºå™¨äººåˆå§‹åŒ–å®Œæˆ (æ€è€ƒæ¨¡å¼: {'å¯ç”¨' if enable_thinking else 'ç¦ç”¨'})")
    
    def format_messages(self, messages: list) -> str:
        """
        æ ¼å¼åŒ–æ¶ˆæ¯ä¸ºæ¨¡å‹è¾“å…¥
        
        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
        
        Returns:
            str: æ ¼å¼åŒ–åçš„è¾“å…¥æ–‡æœ¬
        """
        return self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True,
            enable_thinking=self.enable_thinking
        )
    
    def chat(self, user_input: str, conversation_history: list = None) -> tuple:
        """
        è¿›è¡Œå¯¹è¯
        
        Args:
            user_input: ç”¨æˆ·è¾“å…¥
            conversation_history: å¯¹è¯å†å²
        
        Returns:
            tuple: (å›å¤å†…å®¹, æ€è€ƒè¿‡ç¨‹)
        """
        # æ„å»ºæ¶ˆæ¯å†å²
        messages = conversation_history or []
        messages.append({"role": "user", "content": user_input})
        
        # æ ¼å¼åŒ–è¾“å…¥
        prompt = self.format_messages(messages)
        
        # ç”Ÿæˆå›å¤
        outputs = self.llm.generate([prompt], self.sampling_params)
        generated_text = outputs[0].outputs[0].text
        
        # è§£æå“åº”
        if self.enable_thinking and "<think>" in generated_text and "</think>" in generated_text:
            # åˆ†ç¦»æ€è€ƒè¿‡ç¨‹å’Œæœ€ç»ˆç­”æ¡ˆ
            start_idx = generated_text.find("<think>") + 7
            end_idx = generated_text.find("</think>")
            thinking_content = generated_text[start_idx:end_idx].strip()
            final_answer = generated_text[end_idx + 8:].strip()
            return final_answer, thinking_content
        else:
            # æ¸…ç†å¯èƒ½çš„ç©ºæ€è€ƒæ ‡ç­¾
            if "<think>" in generated_text and "</think>" in generated_text:
                start_idx = generated_text.find("</think>") + 8
                clean_text = generated_text[start_idx:].strip()
            else:
                clean_text = generated_text.strip()
            return clean_text, ""


def demo_single_turn():
    """å•è½®å¯¹è¯æ¼”ç¤º"""
    print("\nğŸ”„ å•è½®å¯¹è¯æ¼”ç¤º")
    print("=" * 40)
    
    # åˆå§‹åŒ–èŠå¤©æœºå™¨äºº
    chatbot = QwenChatBot(enable_thinking=False)
    
    # æµ‹è¯•é—®é¢˜
    questions = [
        "ä½ å¥½ï¼Œä½ æ˜¯è°ï¼Ÿ",
        "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ",
        "è¯·æ¨èå‡ æœ¬å…³äºæœºå™¨å­¦ä¹ çš„ä¹¦ç±ã€‚",
        "å†™ä¸€ä¸ªç®€å•çš„ Python å‡½æ•°æ¥è®¡ç®—ä¸¤ä¸ªæ•°çš„æœ€å¤§å…¬çº¦æ•°ã€‚"
    ]
    
    for i, question in enumerate(questions, 1):
        print(f"\nğŸ“ é—®é¢˜ {i}: {question}")
        print("-" * 30)
        
        try:
            answer, _ = chatbot.chat(question)
            print(f"ğŸ¤– å›ç­”: {answer}")
        except Exception as e:
            print(f"âŒ é”™è¯¯: {e}")


def demo_multi_turn():
    """å¤šè½®å¯¹è¯æ¼”ç¤º"""
    print("\nğŸ”„ å¤šè½®å¯¹è¯æ¼”ç¤º")
    print("=" * 40)
    
    # åˆå§‹åŒ–èŠå¤©æœºå™¨äºº
    chatbot = QwenChatBot(enable_thinking=False)
    
    # å¯¹è¯å†å²
    conversation = []
    
    # æ¨¡æ‹Ÿå¤šè½®å¯¹è¯
    turns = [
        "æˆ‘æƒ³å­¦ä¹ æ·±åº¦å­¦ä¹ ï¼Œåº”è¯¥ä»å“ªé‡Œå¼€å§‹ï¼Ÿ",
        "æˆ‘å·²ç»æœ‰ä¸€äº› Python åŸºç¡€äº†ï¼Œè¿˜éœ€è¦å­¦ä¹ ä»€ä¹ˆæ•°å­¦çŸ¥è¯†ï¼Ÿ",
        "æ¨èä¸€äº›å®è·µé¡¹ç›®å§ã€‚",
        "è°¢è°¢ä½ çš„å»ºè®®ï¼"
    ]
    
    for i, user_input in enumerate(turns, 1):
        print(f"\nğŸ’¬ ç¬¬ {i} è½®å¯¹è¯")
        print(f"ğŸ‘¤ ç”¨æˆ·: {user_input}")
        
        try:
            # è¿›è¡Œå¯¹è¯
            answer, _ = chatbot.chat(user_input, conversation.copy())
            print(f"ğŸ¤– åŠ©æ‰‹: {answer}")
            
            # æ›´æ–°å¯¹è¯å†å²
            conversation.append({"role": "user", "content": user_input})
            conversation.append({"role": "assistant", "content": answer})
            
        except Exception as e:
            print(f"âŒ é”™è¯¯: {e}")


def demo_thinking_mode():
    """æ€è€ƒæ¨¡å¼æ¼”ç¤º"""
    print("\nğŸ§  æ€è€ƒæ¨¡å¼æ¼”ç¤º")
    print("=" * 40)
    
    # åˆå§‹åŒ–æ€è€ƒæ¨¡å¼èŠå¤©æœºå™¨äºº
    chatbot = QwenChatBot(enable_thinking=True)
    
    # éœ€è¦æ¨ç†çš„é—®é¢˜
    questions = [
        "è®¡ç®— 6 çš„é˜¶ä¹˜æ˜¯å¤šå°‘ï¼Ÿ",
        "å¦‚æœä¸€ä¸ªæ­£æ–¹å½¢çš„é¢ç§¯æ˜¯ 25ï¼Œé‚£ä¹ˆå®ƒçš„å‘¨é•¿æ˜¯å¤šå°‘ï¼Ÿ",
        "è§£é‡Šä¸€ä¸‹ä¸ºä»€ä¹ˆæ·±åº¦å­¦ä¹ åœ¨å›¾åƒè¯†åˆ«æ–¹é¢å¦‚æ­¤æœ‰æ•ˆï¼Ÿ"
    ]
    
    for i, question in enumerate(questions, 1):
        print(f"\nğŸ“ é—®é¢˜ {i}: {question}")
        print("-" * 30)
        
        try:
            answer, thinking = chatbot.chat(question)
            
            if thinking:
                print(f"ğŸ’­ æ€è€ƒè¿‡ç¨‹: {thinking[:200]}...")
                print("-" * 20)
            
            print(f"âœ… æœ€ç»ˆç­”æ¡ˆ: {answer}")
            
        except Exception as e:
            print(f"âŒ é”™è¯¯: {e}")


def interactive_chat():
    """äº¤äº’å¼å¯¹è¯"""
    print("\nğŸ’¬ äº¤äº’å¼å¯¹è¯æ¨¡å¼")
    print("=" * 40)
    print("è¾“å…¥ 'quit' é€€å‡ºï¼Œè¾“å…¥ 'clear' æ¸…ç©ºå†å²ï¼Œè¾“å…¥ 'thinking' åˆ‡æ¢æ€è€ƒæ¨¡å¼")
    
    # åˆå§‹åŒ–èŠå¤©æœºå™¨äºº
    enable_thinking = False
    chatbot = QwenChatBot(enable_thinking=enable_thinking)
    conversation = []
    
    while True:
        try:
            user_input = input(f"\nğŸ‘¤ ç”¨æˆ· ({'æ€è€ƒæ¨¡å¼' if enable_thinking else 'æ™®é€šæ¨¡å¼'}): ").strip()
            
            if user_input.lower() in ['quit', 'exit', 'é€€å‡º', 'q']:
                print("ğŸ‘‹ å†è§ï¼")
                break
            elif user_input.lower() in ['clear', 'æ¸…ç©º']:
                conversation = []
                print("ğŸ—‘ï¸  å¯¹è¯å†å²å·²æ¸…ç©º")
                continue
            elif user_input.lower() in ['thinking', 'æ€è€ƒ']:
                enable_thinking = not enable_thinking
                chatbot = QwenChatBot(enable_thinking=enable_thinking)
                print(f"ğŸ”„ å·²åˆ‡æ¢åˆ°{'æ€è€ƒæ¨¡å¼' if enable_thinking else 'æ™®é€šæ¨¡å¼'}")
                continue
            elif not user_input:
                continue
            
            # è¿›è¡Œå¯¹è¯
            answer, thinking = chatbot.chat(user_input, conversation.copy())
            
            if thinking and enable_thinking:
                print(f"\nğŸ’­ æ€è€ƒè¿‡ç¨‹: {thinking[:200]}...")
                print("-" * 20)
            
            print(f"\nğŸ¤– åŠ©æ‰‹: {answer}")
            
            # æ›´æ–°å¯¹è¯å†å²
            conversation.append({"role": "user", "content": user_input})
            conversation.append({"role": "assistant", "content": answer})
            
            # é™åˆ¶å†å²é•¿åº¦
            if len(conversation) > 20:
                conversation = conversation[-20:]
                
        except KeyboardInterrupt:
            print("\nğŸ‘‹ å†è§ï¼")
            break
        except Exception as e:
            print(f"âŒ å¯¹è¯è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¤– Qwen3-8B åŸºç¡€å¯¹è¯ç¤ºä¾‹")
    print("=" * 50)
    
    try:
        # è¿è¡Œæ¼”ç¤º
        demo_single_turn()
        demo_multi_turn()
        demo_thinking_mode()
        
        # è¯¢é—®æ˜¯å¦è¿›å…¥äº¤äº’æ¨¡å¼
        choice = input("\nâ“ æ˜¯å¦è¿›å…¥äº¤äº’å¼å¯¹è¯æ¨¡å¼ï¼Ÿ(y/n): ").strip().lower()
        if choice in ['y', 'yes', 'æ˜¯']:
            interactive_chat()
            
    except Exception as e:
        print(f"âŒ ç¨‹åºè¿è¡Œå‡ºé”™: {e}")
    
    print("\nğŸ‰ åŸºç¡€å¯¹è¯ç¤ºä¾‹å®Œæˆï¼")


if __name__ == "__main__":
    main()
