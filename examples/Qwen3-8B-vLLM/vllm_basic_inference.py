"""
Qwen3-8B vLLM åŸºç¡€æ¨ç†ç¤ºä¾‹

æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨ vLLM è¿›è¡ŒåŸºç¡€çš„æ–‡æœ¬ç”Ÿæˆæ¨ç†
è¿™æ˜¯æœ€ç®€å•çš„ä½¿ç”¨ç¤ºä¾‹ï¼Œé€‚åˆåˆå­¦è€…
"""

import os
from pathlib import Path
from vllm import LLM, SamplingParams
from transformers import AutoTokenizer


def get_model_path() -> str:
    """è·å–æ¨¡å‹è·¯å¾„"""
    # ä¼˜å…ˆæ£€æŸ¥ AutoDL ç¯å¢ƒ
    autodl_path = "/root/autodl-tmp/Qwen/Qwen3-8B"
    if os.path.exists(autodl_path):
        return autodl_path
    
    # æ£€æŸ¥æœ¬åœ° models ç›®å½•
    local_path = "./models/Qwen/Qwen3-8B"
    if os.path.exists(local_path):
        return local_path
    
    # å¦‚æœéƒ½ä¸å­˜åœ¨ï¼Œè¿”å›é»˜è®¤è·¯å¾„å¹¶æç¤ºç”¨æˆ·
    print("âš ï¸  æœªæ‰¾åˆ°æ¨¡å‹æ–‡ä»¶ï¼Œè¯·å…ˆè¿è¡Œ model_download.py ä¸‹è½½æ¨¡å‹")
    return local_path


def basic_text_generation():
    """åŸºç¡€æ–‡æœ¬ç”Ÿæˆç¤ºä¾‹"""
    print("ğŸš€ åŸºç¡€æ–‡æœ¬ç”Ÿæˆç¤ºä¾‹")
    print("=" * 40)
    
    # è®¾ç½®ç¯å¢ƒå˜é‡
    os.environ['VLLM_USE_MODELSCOPE'] = 'True'
    
    # è·å–æ¨¡å‹è·¯å¾„
    model_path = get_model_path()
    print(f"ğŸ“ æ¨¡å‹è·¯å¾„: {model_path}")
    
    # åˆå§‹åŒ– vLLM å¼•æ“
    print("â³ åˆå§‹åŒ– vLLM å¼•æ“...")
    llm = LLM(
        model=model_path,
        max_model_len=8192,
        trust_remote_code=True
    )
    print("âœ… vLLM å¼•æ“åˆå§‹åŒ–å®Œæˆ")
    
    # è®¾ç½®é‡‡æ ·å‚æ•°
    sampling_params = SamplingParams(
        temperature=0.7,
        top_p=0.8,
        top_k=20,
        max_tokens=512,
        stop_token_ids=[151645, 151643]
    )
    
    # æµ‹è¯•æç¤ºè¯
    prompts = [
        "äººå·¥æ™ºèƒ½æ˜¯",
        "æ·±åº¦å­¦ä¹ çš„ä¸»è¦ä¼˜åŠ¿åŒ…æ‹¬",
        "Python æ˜¯ä¸€ç§",
        "æœºå™¨å­¦ä¹ åœ¨åŒ»ç–—é¢†åŸŸçš„åº”ç”¨æœ‰"
    ]
    
    print("\nğŸ“ å¼€å§‹æ–‡æœ¬ç”Ÿæˆ...")
    
    # ç”Ÿæˆæ–‡æœ¬
    outputs = llm.generate(prompts, sampling_params)
    
    # æ˜¾ç¤ºç»“æœ
    for i, output in enumerate(outputs):
        prompt = prompts[i]
        generated_text = output.outputs[0].text
        
        print(f"\nğŸ” æç¤ºè¯ {i+1}: {prompt}")
        print(f"ğŸ¤– ç”Ÿæˆæ–‡æœ¬: {generated_text}")
        print(f"ğŸ ç»“æŸåŸå› : {output.outputs[0].finish_reason}")
        print("-" * 40)


def chat_format_generation():
    """å¯¹è¯æ ¼å¼ç”Ÿæˆç¤ºä¾‹"""
    print("\nğŸ’¬ å¯¹è¯æ ¼å¼ç”Ÿæˆç¤ºä¾‹")
    print("=" * 40)
    
    # è®¾ç½®ç¯å¢ƒå˜é‡
    os.environ['VLLM_USE_MODELSCOPE'] = 'True'
    
    # è·å–æ¨¡å‹è·¯å¾„
    model_path = get_model_path()
    
    # åŠ è½½åˆ†è¯å™¨
    print("ğŸ“ åŠ è½½åˆ†è¯å™¨...")
    tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)
    
    # åˆå§‹åŒ– vLLM å¼•æ“
    print("â³ åˆå§‹åŒ– vLLM å¼•æ“...")
    llm = LLM(
        model=model_path,
        max_model_len=8192,
        trust_remote_code=True
    )
    
    # è®¾ç½®é‡‡æ ·å‚æ•°
    sampling_params = SamplingParams(
        temperature=0.7,
        top_p=0.8,
        top_k=20,
        max_tokens=1024,
        stop_token_ids=[151645, 151643]
    )
    
    # å‡†å¤‡å¯¹è¯æ•°æ®
    conversations = [
        [
            {"role": "user", "content": "ä½ å¥½ï¼Œä½ æ˜¯è°ï¼Ÿ"}
        ],
        [
            {"role": "user", "content": "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ"}
        ],
        [
            {"role": "user", "content": "å†™ä¸€ä¸ª Python å‡½æ•°æ¥è®¡ç®—æ–æ³¢é‚£å¥‘æ•°åˆ—"}
        ]
    ]
    
    # æ ¼å¼åŒ–å¯¹è¯ä¸ºæ¨¡å‹è¾“å…¥
    prompts = []
    for messages in conversations:
        prompt = tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True,
            enable_thinking=False  # ç¦ç”¨æ€è€ƒæ¨¡å¼
        )
        prompts.append(prompt)
    
    print("ğŸ“ å¼€å§‹å¯¹è¯ç”Ÿæˆ...")
    
    # ç”Ÿæˆå›å¤
    outputs = llm.generate(prompts, sampling_params)
    
    # æ˜¾ç¤ºç»“æœ
    for i, output in enumerate(outputs):
        user_message = conversations[i][0]["content"]
        generated_text = output.outputs[0].text
        
        print(f"\nğŸ‘¤ ç”¨æˆ·: {user_message}")
        print(f"ğŸ¤– åŠ©æ‰‹: {generated_text}")
        print(f"ğŸ ç»“æŸåŸå› : {output.outputs[0].finish_reason}")
        print("-" * 40)


def parameter_comparison():
    """å‚æ•°å¯¹æ¯”ç¤ºä¾‹"""
    print("\nâš™ï¸  å‚æ•°å¯¹æ¯”ç¤ºä¾‹")
    print("=" * 40)
    
    # è®¾ç½®ç¯å¢ƒå˜é‡
    os.environ['VLLM_USE_MODELSCOPE'] = 'True'
    
    # è·å–æ¨¡å‹è·¯å¾„
    model_path = get_model_path()
    
    # åˆå§‹åŒ– vLLM å¼•æ“
    print("â³ åˆå§‹åŒ– vLLM å¼•æ“...")
    llm = LLM(
        model=model_path,
        max_model_len=8192,
        trust_remote_code=True
    )
    
    # æµ‹è¯•æç¤ºè¯
    prompt = "å†™ä¸€ä¸ªå…³äºæ˜¥å¤©çš„çŸ­è¯—ï¼š"
    
    # ä¸åŒçš„é‡‡æ ·å‚æ•°é…ç½®
    param_configs = [
        {
            "name": "ä¿å®ˆæ¨¡å¼",
            "params": SamplingParams(
                temperature=0.3,
                top_p=0.8,
                max_tokens=200,
                stop_token_ids=[151645, 151643]
            )
        },
        {
            "name": "å¹³è¡¡æ¨¡å¼", 
            "params": SamplingParams(
                temperature=0.7,
                top_p=0.9,
                max_tokens=200,
                stop_token_ids=[151645, 151643]
            )
        },
        {
            "name": "åˆ›æ„æ¨¡å¼",
            "params": SamplingParams(
                temperature=1.0,
                top_p=0.95,
                max_tokens=200,
                stop_token_ids=[151645, 151643]
            )
        }
    ]
    
    print(f"ğŸ“ æµ‹è¯•æç¤ºè¯: {prompt}")
    print("\nğŸ”„ ä½¿ç”¨ä¸åŒå‚æ•°ç”Ÿæˆ...")
    
    for config in param_configs:
        print(f"\nğŸ›ï¸  {config['name']}:")
        print(f"   Temperature: {config['params'].temperature}")
        print(f"   Top-p: {config['params'].top_p}")
        
        # ç”Ÿæˆæ–‡æœ¬
        outputs = llm.generate([prompt], config['params'])
        generated_text = outputs[0].outputs[0].text
        
        print(f"ğŸ¤– ç”Ÿæˆç»“æœ: {generated_text}")
        print("-" * 30)


def simple_qa_demo():
    """ç®€å•é—®ç­”æ¼”ç¤º"""
    print("\nâ“ ç®€å•é—®ç­”æ¼”ç¤º")
    print("=" * 40)
    
    # è®¾ç½®ç¯å¢ƒå˜é‡
    os.environ['VLLM_USE_MODELSCOPE'] = 'True'
    
    # è·å–æ¨¡å‹è·¯å¾„
    model_path = get_model_path()
    
    # åŠ è½½åˆ†è¯å™¨
    tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)
    
    # åˆå§‹åŒ– vLLM å¼•æ“
    llm = LLM(
        model=model_path,
        max_model_len=8192,
        trust_remote_code=True
    )
    
    # è®¾ç½®é‡‡æ ·å‚æ•°
    sampling_params = SamplingParams(
        temperature=0.7,
        top_p=0.8,
        max_tokens=512,
        stop_token_ids=[151645, 151643]
    )
    
    # é—®ç­”å¯¹
    qa_pairs = [
        "ä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ ï¼Ÿ",
        "Python æœ‰ä»€ä¹ˆä¼˜åŠ¿ï¼Ÿ",
        "å¦‚ä½•å­¦ä¹ æœºå™¨å­¦ä¹ ï¼Ÿ",
        "ä»€ä¹ˆæ˜¯ç¥ç»ç½‘ç»œï¼Ÿ"
    ]
    
    for i, question in enumerate(qa_pairs, 1):
        print(f"\nğŸ“ é—®é¢˜ {i}: {question}")
        
        # æ ¼å¼åŒ–ä¸ºå¯¹è¯æ ¼å¼
        messages = [{"role": "user", "content": question}]
        prompt = tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True,
            enable_thinking=False
        )
        
        # ç”Ÿæˆå›ç­”
        outputs = llm.generate([prompt], sampling_params)
        answer = outputs[0].outputs[0].text
        
        print(f"ğŸ¤– å›ç­”: {answer}")
        print("-" * 30)


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¤– Qwen3-8B vLLM åŸºç¡€æ¨ç†ç¤ºä¾‹")
    print("=" * 50)
    
    try:
        # è¿è¡Œå„ç§åŸºç¡€ç¤ºä¾‹
        basic_text_generation()
        chat_format_generation()
        parameter_comparison()
        simple_qa_demo()
        
    except Exception as e:
        print(f"âŒ ç¨‹åºè¿è¡Œå‡ºé”™: {e}")
        print("ğŸ’¡ è¯·ç¡®ä¿:")
        print("   1. æ¨¡å‹å·²æ­£ç¡®ä¸‹è½½")
        print("   2. æœ‰è¶³å¤Ÿçš„ GPU æ˜¾å­˜")
        print("   3. ä¾èµ–åŒ…å·²æ­£ç¡®å®‰è£…")
    
    print("\nğŸ‰ åŸºç¡€æ¨ç†ç¤ºä¾‹å®Œæˆï¼")
    print("ğŸ’¡ æ¥ä¸‹æ¥å¯ä»¥å°è¯•:")
    print("   - uv run python vllm_thinking_mode.py")
    print("   - uv run python vllm_non_thinking_mode.py")
    print("   - uv run python examples/basic_chat.py")


if __name__ == "__main__":
    main()
