#!/bin/bash

# Qwen3-8B vLLM API æœåŠ¡å™¨å¯åŠ¨è„šæœ¬
# ä½¿ç”¨ vLLM å¯åŠ¨å…¼å®¹ OpenAI API çš„æœåŠ¡å™¨

set -e

# é¢œè‰²å®šä¹‰
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

# é»˜è®¤é…ç½®
DEFAULT_MODEL_PATH=""
DEFAULT_HOST="0.0.0.0"
DEFAULT_PORT="8000"
DEFAULT_MODEL_NAME="Qwen3-8B"
DEFAULT_MAX_LEN="8192"
DEFAULT_GPU_UTIL="0.9"
DEFAULT_PARALLEL_SIZE="1"

# å‡½æ•°ï¼šæ‰“å°å¸¦é¢œè‰²çš„æ¶ˆæ¯
print_info() {
    echo -e "${BLUE}â„¹ï¸  $1${NC}"
}

print_success() {
    echo -e "${GREEN}âœ… $1${NC}"
}

print_warning() {
    echo -e "${YELLOW}âš ï¸  $1${NC}"
}

print_error() {
    echo -e "${RED}âŒ $1${NC}"
}

# å‡½æ•°ï¼šæ£€æµ‹æ¨¡å‹è·¯å¾„
detect_model_path() {
    # ä¼˜å…ˆæ£€æŸ¥ AutoDL ç¯å¢ƒ
    if [ -d "/root/autodl-tmp/Qwen/Qwen3-8B" ]; then
        echo "/root/autodl-tmp/Qwen/Qwen3-8B"
        return 0
    fi
    
    # æ£€æŸ¥æœ¬åœ° models ç›®å½•
    if [ -d "./models/Qwen/Qwen3-8B" ]; then
        echo "./models/Qwen/Qwen3-8B"
        return 0
    fi
    
    # æ£€æŸ¥ç›¸å¯¹è·¯å¾„
    if [ -d "../models/Qwen/Qwen3-8B" ]; then
        echo "../models/Qwen/Qwen3-8B"
        return 0
    fi
    
    return 1
}

# å‡½æ•°ï¼šæ£€æŸ¥æ¨¡å‹æ–‡ä»¶
check_model_files() {
    local model_path="$1"
    
    if [ ! -d "$model_path" ]; then
        print_error "æ¨¡å‹ç›®å½•ä¸å­˜åœ¨: $model_path"
        return 1
    fi
    
    # æ£€æŸ¥å…³é”®æ–‡ä»¶
    local required_files=("config.json" "tokenizer.json" "tokenizer_config.json")
    
    for file in "${required_files[@]}"; do
        if [ ! -f "$model_path/$file" ]; then
            print_error "ç¼ºå°‘æ¨¡å‹æ–‡ä»¶: $file"
            return 1
        fi
    done
    
    return 0
}

# å‡½æ•°ï¼šæ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯
show_help() {
    echo "Qwen3-8B vLLM API æœåŠ¡å™¨å¯åŠ¨è„šæœ¬"
    echo ""
    echo "ç”¨æ³•: $0 [é€‰é¡¹]"
    echo ""
    echo "é€‰é¡¹:"
    echo "  -m, --model-path PATH        æ¨¡å‹è·¯å¾„ (é»˜è®¤è‡ªåŠ¨æ£€æµ‹)"
    echo "  -h, --host HOST              æœåŠ¡å™¨ä¸»æœºåœ°å€ (é»˜è®¤: $DEFAULT_HOST)"
    echo "  -p, --port PORT              æœåŠ¡å™¨ç«¯å£ (é»˜è®¤: $DEFAULT_PORT)"
    echo "  -n, --model-name NAME        æœåŠ¡æ¨¡å‹åç§° (é»˜è®¤: $DEFAULT_MODEL_NAME)"
    echo "  -l, --max-len LENGTH         æ¨¡å‹æœ€å¤§é•¿åº¦ (é»˜è®¤: $DEFAULT_MAX_LEN)"
    echo "  -g, --gpu-util RATIO         GPU å†…å­˜åˆ©ç”¨ç‡ (é»˜è®¤: $DEFAULT_GPU_UTIL)"
    echo "  -t, --tensor-parallel SIZE   å¼ é‡å¹¶è¡Œå¤§å° (é»˜è®¤: $DEFAULT_PARALLEL_SIZE)"
    echo "  --disable-reasoning          ç¦ç”¨æ¨ç†æ¨¡å¼"
    echo "  --help                       æ˜¾ç¤ºæ­¤å¸®åŠ©ä¿¡æ¯"
    echo ""
    echo "ç¤ºä¾‹:"
    echo "  $0                                    # ä½¿ç”¨é»˜è®¤é…ç½®å¯åŠ¨"
    echo "  $0 -p 8080                           # åœ¨ç«¯å£ 8080 å¯åŠ¨"
    echo "  $0 -m /path/to/model --disable-reasoning  # æŒ‡å®šæ¨¡å‹è·¯å¾„å¹¶ç¦ç”¨æ¨ç†æ¨¡å¼"
}

# è§£æå‘½ä»¤è¡Œå‚æ•°
MODEL_PATH=""
HOST="$DEFAULT_HOST"
PORT="$DEFAULT_PORT"
MODEL_NAME="$DEFAULT_MODEL_NAME"
MAX_LEN="$DEFAULT_MAX_LEN"
GPU_UTIL="$DEFAULT_GPU_UTIL"
PARALLEL_SIZE="$DEFAULT_PARALLEL_SIZE"
ENABLE_REASONING=true

while [[ $# -gt 0 ]]; do
    case $1 in
        -m|--model-path)
            MODEL_PATH="$2"
            shift 2
            ;;
        -h|--host)
            HOST="$2"
            shift 2
            ;;
        -p|--port)
            PORT="$2"
            shift 2
            ;;
        -n|--model-name)
            MODEL_NAME="$2"
            shift 2
            ;;
        -l|--max-len)
            MAX_LEN="$2"
            shift 2
            ;;
        -g|--gpu-util)
            GPU_UTIL="$2"
            shift 2
            ;;
        -t|--tensor-parallel)
            PARALLEL_SIZE="$2"
            shift 2
            ;;
        --disable-reasoning)
            ENABLE_REASONING=false
            shift
            ;;
        --help)
            show_help
            exit 0
            ;;
        *)
            print_error "æœªçŸ¥é€‰é¡¹: $1"
            show_help
            exit 1
            ;;
    esac
done

# æ£€æµ‹æ¨¡å‹è·¯å¾„
if [ -z "$MODEL_PATH" ]; then
    print_info "è‡ªåŠ¨æ£€æµ‹æ¨¡å‹è·¯å¾„..."
    if MODEL_PATH=$(detect_model_path); then
        print_success "æ‰¾åˆ°æ¨¡å‹è·¯å¾„: $MODEL_PATH"
    else
        print_error "æœªæ‰¾åˆ°æ¨¡å‹æ–‡ä»¶"
        print_info "è¯·å…ˆä¸‹è½½æ¨¡å‹:"
        print_info "  uv run python model_download.py"
        exit 1
    fi
fi

# æ£€æŸ¥æ¨¡å‹æ–‡ä»¶
print_info "æ£€æŸ¥æ¨¡å‹æ–‡ä»¶..."
if ! check_model_files "$MODEL_PATH"; then
    print_error "æ¨¡å‹æ–‡ä»¶æ£€æŸ¥å¤±è´¥"
    exit 1
fi
print_success "æ¨¡å‹æ–‡ä»¶æ£€æŸ¥é€šè¿‡"

# è®¾ç½®ç¯å¢ƒå˜é‡
export VLLM_USE_MODELSCOPE=true

# æ„å»ºå¯åŠ¨å‘½ä»¤
CMD="vllm serve \"$MODEL_PATH\""
CMD="$CMD --host $HOST"
CMD="$CMD --port $PORT"
CMD="$CMD --served-model-name $MODEL_NAME"
CMD="$CMD --max-model-len $MAX_LEN"
CMD="$CMD --gpu-memory-utilization $GPU_UTIL"
CMD="$CMD --tensor-parallel-size $PARALLEL_SIZE"
CMD="$CMD --trust-remote-code"

if [ "$ENABLE_REASONING" = true ]; then
    CMD="$CMD --enable-reasoning --reasoning-parser deepseek_r1"
fi

# æ˜¾ç¤ºå¯åŠ¨ä¿¡æ¯
echo ""
print_info "ğŸš€ å¯åŠ¨ vLLM API æœåŠ¡å™¨"
echo "=================================================="
print_info "ğŸ“ æ¨¡å‹è·¯å¾„: $MODEL_PATH"
print_info "ğŸŒ æœåŠ¡åœ°å€: http://$HOST:$PORT"
print_info "ğŸ·ï¸  æ¨¡å‹åç§°: $MODEL_NAME"
print_info "ğŸ“ æœ€å¤§é•¿åº¦: $MAX_LEN"
print_info "ğŸ§  æ¨ç†æ¨¡å¼: $([ "$ENABLE_REASONING" = true ] && echo "å¯ç”¨" || echo "ç¦ç”¨")"
print_info "ğŸ¯ GPU åˆ©ç”¨ç‡: $GPU_UTIL"
print_info "âš¡ å¹¶è¡Œå¤§å°: $PARALLEL_SIZE"
echo "=================================================="

print_info "æ‰§è¡Œå‘½ä»¤:"
echo "$CMD"
echo ""

print_info "â³ æ­£åœ¨å¯åŠ¨æœåŠ¡å™¨..."
print_info "ğŸ’¡ æç¤º: æœåŠ¡å™¨å¯åŠ¨åï¼Œå¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼æµ‹è¯•:"
print_info "   curl http://localhost:$PORT/v1/models"
print_info "   uv run python test_openai_chat_completions.py"
echo ""
print_warning "æŒ‰ Ctrl+C åœæ­¢æœåŠ¡å™¨"
echo "--------------------------------------------------"

# å¯åŠ¨æœåŠ¡å™¨
eval $CMD
