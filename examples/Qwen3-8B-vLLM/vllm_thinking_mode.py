"""
Qwen3-8B vLLM æ€è€ƒæ¨¡å¼æ¨ç†ç¤ºä¾‹

æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨ vLLM è¿›è¡Œ Qwen3-8B æ¨¡å‹çš„æ€è€ƒæ¨¡å¼æ¨ç†
æ€è€ƒæ¨¡å¼ä¼šæ˜¾ç¤ºæ¨¡å‹çš„æ¨ç†è¿‡ç¨‹ï¼Œç±»ä¼¼äº QwQ-32B
"""

import os
from pathlib import Path
from vllm import LLM, SamplingParams
from transformers import AutoTokenizer


def get_model_path() -> str:
    """è·å–æ¨¡å‹è·¯å¾„"""
    # ä¼˜å…ˆæ£€æŸ¥ AutoDL ç¯å¢ƒ
    autodl_path = "/root/autodl-tmp/Qwen/Qwen3-8B"
    if os.path.exists(autodl_path):
        return autodl_path
    
    # æ£€æŸ¥æœ¬åœ° models ç›®å½•
    local_path = "./models/Qwen/Qwen3-8B"
    if os.path.exists(local_path):
        return local_path
    
    # å¦‚æœéƒ½ä¸å­˜åœ¨ï¼Œè¿”å›é»˜è®¤è·¯å¾„å¹¶æç¤ºç”¨æˆ·
    print("âš ï¸  æœªæ‰¾åˆ°æ¨¡å‹æ–‡ä»¶ï¼Œè¯·å…ˆè¿è¡Œ model_download.py ä¸‹è½½æ¨¡å‹")
    return local_path


def create_thinking_prompt(user_input: str) -> str:
    """
    åˆ›å»ºæ€è€ƒæ¨¡å¼çš„æç¤ºè¯
    
    Args:
        user_input: ç”¨æˆ·è¾“å…¥
    
    Returns:
        str: æ ¼å¼åŒ–åçš„æç¤ºè¯
    """
    messages = [
        {"role": "user", "content": user_input}
    ]
    
    # è·å–æ¨¡å‹è·¯å¾„å¹¶åŠ è½½åˆ†è¯å™¨
    model_path = get_model_path()
    tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)
    
    # åº”ç”¨èŠå¤©æ¨¡æ¿ï¼Œå¯ç”¨æ€è€ƒæ¨¡å¼
    text = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True,
        enable_thinking=True  # å¯ç”¨æ€è€ƒæ¨¡å¼
    )
    
    return text


def get_completion_thinking(
    prompts: list,
    model_path: str,
    temperature: float = 0.6,
    top_p: float = 0.95,
    top_k: int = 20,
    min_p: float = 0,
    max_tokens: int = 4096,
    max_model_len: int = 8192
):
    """
    ä½¿ç”¨æ€è€ƒæ¨¡å¼è¿›è¡Œæ–‡æœ¬ç”Ÿæˆ
    
    Args:
        prompts: æç¤ºè¯åˆ—è¡¨
        model_path: æ¨¡å‹è·¯å¾„
        temperature: æ¸©åº¦å‚æ•°ï¼Œæ§åˆ¶ç”Ÿæˆæ–‡æœ¬çš„éšæœºæ€§
        top_p: æ ¸å¿ƒé‡‡æ ·æ¦‚ç‡
        top_k: å€™é€‰è¯æ•°é‡é™åˆ¶
        min_p: æœ€å°æ¦‚ç‡é˜ˆå€¼
        max_tokens: æœ€å¤§ç”Ÿæˆé•¿åº¦
        max_model_len: æ¨¡å‹æœ€å¤§é•¿åº¦
    
    Returns:
        ç”Ÿæˆç»“æœåˆ—è¡¨
    """
    # è®¾ç½®åœæ­¢è¯ ID
    stop_token_ids = [151645, 151643]
    
    # åˆ›å»ºé‡‡æ ·å‚æ•° - æ€è€ƒæ¨¡å¼æ¨èå‚æ•°
    sampling_params = SamplingParams(
        temperature=temperature,
        top_p=top_p,
        top_k=top_k,
        min_p=min_p,
        max_tokens=max_tokens,
        stop_token_ids=stop_token_ids
    )
    
    # åˆå§‹åŒ– vLLM æ¨ç†å¼•æ“
    llm = LLM(
        model=model_path,
        max_model_len=max_model_len,
        trust_remote_code=True
    )
    
    # ç”Ÿæˆæ–‡æœ¬
    outputs = llm.generate(prompts, sampling_params)
    return outputs


def parse_thinking_response(response_text: str) -> tuple:
    """
    è§£ææ€è€ƒæ¨¡å¼çš„å“åº”ï¼Œåˆ†ç¦»æ€è€ƒè¿‡ç¨‹å’Œæœ€ç»ˆç­”æ¡ˆ
    
    Args:
        response_text: æ¨¡å‹å“åº”æ–‡æœ¬
    
    Returns:
        tuple: (æ€è€ƒè¿‡ç¨‹, æœ€ç»ˆç­”æ¡ˆ)
    """
    if "<think>" in response_text and "</think>" in response_text:
        # æå–æ€è€ƒè¿‡ç¨‹
        start_idx = response_text.find("<think>") + 7
        end_idx = response_text.find("</think>")
        thinking_content = response_text[start_idx:end_idx].strip()
        
        # æå–æœ€ç»ˆç­”æ¡ˆ
        final_answer = response_text[end_idx + 8:].strip()
        
        return thinking_content, final_answer
    else:
        # å¦‚æœæ²¡æœ‰æ€è€ƒæ ‡ç­¾ï¼Œæ•´ä¸ªå“åº”ä½œä¸ºæœ€ç»ˆç­”æ¡ˆ
        return "", response_text.strip()


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¤– Qwen3-8B vLLM æ€è€ƒæ¨¡å¼æ¨ç†ç¤ºä¾‹")
    print("=" * 50)
    
    # è®¾ç½®ç¯å¢ƒå˜é‡
    os.environ['VLLM_USE_MODELSCOPE'] = 'True'
    
    # è·å–æ¨¡å‹è·¯å¾„
    model_path = get_model_path()
    print(f"ğŸ“ æ¨¡å‹è·¯å¾„: {model_path}")
    
    # æµ‹è¯•é—®é¢˜åˆ—è¡¨
    test_questions = [
        "ç»™æˆ‘ä¸€ä¸ªå…³äºå¤§æ¨¡å‹çš„ç®€çŸ­ä»‹ç»ã€‚",
        "5çš„é˜¶ä¹˜æ˜¯å¤šå°‘ï¼Ÿè¯·è¯¦ç»†è¯´æ˜è®¡ç®—è¿‡ç¨‹ã€‚",
        "ä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ ï¼Ÿ",
        "è¯·è§£é‡Šä¸€ä¸‹ Transformer æ¶æ„çš„æ ¸å¿ƒæ€æƒ³ã€‚"
    ]
    
    for i, question in enumerate(test_questions, 1):
        print(f"\nğŸ” æµ‹è¯•é—®é¢˜ {i}: {question}")
        print("-" * 40)
        
        try:
            # åˆ›å»ºæ€è€ƒæ¨¡å¼æç¤ºè¯
            prompt = create_thinking_prompt(question)
            
            # è¿›è¡Œæ¨ç†
            print("â³ æ­£åœ¨ç”Ÿæˆå›ç­”...")
            outputs = get_completion_thinking([prompt], model_path)
            
            # è§£æç»“æœ
            for output in outputs:
                generated_text = output.outputs[0].text
                thinking_content, final_answer = parse_thinking_response(generated_text)
                
                if thinking_content:
                    print(f"\nğŸ’­ æ€è€ƒè¿‡ç¨‹:")
                    print(thinking_content[:500] + "..." if len(thinking_content) > 500 else thinking_content)
                
                print(f"\nâœ… æœ€ç»ˆç­”æ¡ˆ:")
                print(final_answer)
                
        except Exception as e:
            print(f"âŒ æ¨ç†è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
    
    print("\n" + "=" * 50)
    print("ğŸ‰ æ€è€ƒæ¨¡å¼æ¨ç†ç¤ºä¾‹å®Œæˆï¼")


if __name__ == "__main__":
    main()
