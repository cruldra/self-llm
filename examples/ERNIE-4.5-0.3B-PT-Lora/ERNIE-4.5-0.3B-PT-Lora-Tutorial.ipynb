{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ERNIE-4.5-0.3B-PT LoRA 微调教程\n",
    "\n",
    "本教程演示如何使用 LoRA 技术对 ERNIE-4.5-0.3B-PT 模型进行微调，构建一个能够模拟甄嬛对话风格的个性化 LLM。\n",
    "\n",
    "## 环境要求\n",
    "- Python 3.12+\n",
    "- CUDA 12.4+\n",
    "- PyTorch 2.5.1+\n",
    "- 显存需求：约 24GB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 导入必要的库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 库导入成功\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM, \n",
    "    TrainingArguments, \n",
    "    Trainer, \n",
    "    DataCollatorForSeq2Seq\n",
    ")\n",
    "from peft import LoraConfig, TaskType, get_peft_model, PeftModel\n",
    "import swanlab\n",
    "from swanlab.integration.transformers import SwanLabCallback\n",
    "from config import Paths, LoRAConfig, TrainingConfig, SwanLabConfig, InferenceConfig\n",
    "\n",
    "print(\"✅ 库导入成功\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 检查环境和路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型路径: d:\\Sources\\self-llm\\examples\\ERNIE-4.5-0.3B-PT-Lora\\models\\PaddlePaddle\\ERNIE-4___5-0___3B-PT\n",
      "模型存在: True\n",
      "数据集路径: d:\\Sources\\self-llm\\examples\\ERNIE-4.5-0.3B-PT-Lora\\..\\..\\dataset\\huanhuan.json\n",
      "数据集存在: True\n",
      "CUDA 可用: True\n",
      "GPU 数量: 1\n",
      "当前 GPU: NVIDIA GeForce RTX 2080 Ti\n"
     ]
    }
   ],
   "source": [
    "# 检查模型路径\n",
    "print(f\"模型路径: {Paths.MODEL_PATH}\")\n",
    "print(f\"模型存在: {os.path.exists(Paths.MODEL_PATH)}\")\n",
    "\n",
    "# 检查数据集路径\n",
    "print(f\"数据集路径: {Paths.DATASET_PATH}\")\n",
    "print(f\"数据集存在: {os.path.exists(Paths.DATASET_PATH)}\")\n",
    "\n",
    "# 检查 CUDA\n",
    "print(f\"CUDA 可用: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU 数量: {torch.cuda.device_count()}\")\n",
    "    print(f\"当前 GPU: {torch.cuda.get_device_name()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 加载数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "加载数据集: d:\\Sources\\self-llm\\examples\\ERNIE-4.5-0.3B-PT-Lora\\..\\..\\dataset\\huanhuan.json\n",
      "数据集大小: 3729\n",
      "\n",
      "数据集样例:\n",
      "样例 1:\n",
      "  指令: 小姐，别的秀女都在求中选，唯有咱们小姐想被撂牌子，菩萨一定记得真真儿的——\n",
      "  输出: 嘘——都说许愿说破是不灵的。\n",
      "\n",
      "样例 2:\n",
      "  指令: 这个温太医啊，也是古怪，谁不知太医不得皇命不能为皇族以外的人请脉诊病，他倒好，十天半月便往咱们府里跑。\n",
      "  输出: 你们俩话太多了，我该和温太医要一剂药，好好治治你们。\n",
      "\n",
      "样例 3:\n",
      "  指令: 嬛妹妹，刚刚我去府上请脉，听甄伯母说你来这里进香了。\n",
      "  输出: 出来走走，也是散心。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def load_dataset(data_path):\n",
    "    \"\"\"加载甄嬛数据集\"\"\"\n",
    "    print(f\"加载数据集: {data_path}\")\n",
    "    with open(data_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    print(f\"数据集大小: {len(data)}\")\n",
    "    return Dataset.from_list(data)\n",
    "\n",
    "# 加载数据集\n",
    "dataset = load_dataset(Paths.DATASET_PATH)\n",
    "\n",
    "# 查看数据集样例\n",
    "print(\"\\n数据集样例:\")\n",
    "for i in range(min(3, len(dataset))):\n",
    "    print(f\"样例 {i+1}:\")\n",
    "    print(f\"  指令: {dataset[i]['instruction']}\")\n",
    "    print(f\"  输出: {dataset[i]['output']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 加载模型和 Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "加载 tokenizer...\n",
      "✅ tokenizer 加载成功\n",
      "\n",
      "加载模型...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The checkpoint you are trying to load has model type `ernie4_5` but Transformers does not recognize this architecture. This could be because of an issue with the checkpoint, or because your version of Transformers is out of date.\n\nYou can update Transformers with the command `pip install --upgrade transformers`. If this does not work, and the checkpoint is very new, then there may not be a release version that supports this model yet. In this case, you can get the most up-to-date code by installing Transformers from source with the command `pip install git+https://github.com/huggingface/transformers.git`",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Sources\\self-llm\\examples\\ERNIE-4.5-0.3B-PT-Lora\\.venv\\Lib\\site-packages\\transformers\\models\\auto\\configuration_auto.py:1218\u001b[39m, in \u001b[36mAutoConfig.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[39m\n\u001b[32m   1217\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1218\u001b[39m     config_class = \u001b[43mCONFIG_MAPPING\u001b[49m\u001b[43m[\u001b[49m\u001b[43mconfig_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel_type\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m   1219\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Sources\\self-llm\\examples\\ERNIE-4.5-0.3B-PT-Lora\\.venv\\Lib\\site-packages\\transformers\\models\\auto\\configuration_auto.py:914\u001b[39m, in \u001b[36m_LazyConfigMapping.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m    913\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._mapping:\n\u001b[32m--> \u001b[39m\u001b[32m914\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n\u001b[32m    915\u001b[39m value = \u001b[38;5;28mself\u001b[39m._mapping[key]\n",
      "\u001b[31mKeyError\u001b[39m: 'ernie4_5'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# 加载模型\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m加载模型...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m model = \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mPaths\u001b[49m\u001b[43m.\u001b[49m\u001b[43mMODEL_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mauto\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbfloat16\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m     17\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m✅ 模型加载成功\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# 启用梯度检查点\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Sources\\self-llm\\examples\\ERNIE-4.5-0.3B-PT-Lora\\.venv\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:547\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    544\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mquantization_config\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    545\u001b[39m     _ = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mquantization_config\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m547\u001b[39m config, kwargs = \u001b[43mAutoConfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    548\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    549\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_unused_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    550\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcode_revision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcode_revision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    551\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    552\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    553\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    554\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    556\u001b[39m \u001b[38;5;66;03m# if torch_dtype=auto was passed here, ensure to pass it on\u001b[39;00m\n\u001b[32m    557\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m kwargs_orig.get(\u001b[33m\"\u001b[39m\u001b[33mtorch_dtype\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) == \u001b[33m\"\u001b[39m\u001b[33mauto\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Sources\\self-llm\\examples\\ERNIE-4.5-0.3B-PT-Lora\\.venv\\Lib\\site-packages\\transformers\\models\\auto\\configuration_auto.py:1220\u001b[39m, in \u001b[36mAutoConfig.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[39m\n\u001b[32m   1218\u001b[39m         config_class = CONFIG_MAPPING[config_dict[\u001b[33m\"\u001b[39m\u001b[33mmodel_type\u001b[39m\u001b[33m\"\u001b[39m]]\n\u001b[32m   1219\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1220\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1221\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe checkpoint you are trying to load has model type `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig_dict[\u001b[33m'\u001b[39m\u001b[33mmodel_type\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m` \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1222\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mbut Transformers does not recognize this architecture. This could be because of an \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1223\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33missue with the checkpoint, or because your version of Transformers is out of date.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1224\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mYou can update Transformers with the command `pip install --upgrade transformers`. If this \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1225\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mdoes not work, and the checkpoint is very new, then there may not be a release version \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1226\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mthat supports this model yet. In this case, you can get the most up-to-date code by installing \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1227\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mTransformers from source with the command \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1228\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m`pip install git+https://github.com/huggingface/transformers.git`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1229\u001b[39m         )\n\u001b[32m   1230\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m config_class.from_dict(config_dict, **unused_kwargs)\n\u001b[32m   1231\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1232\u001b[39m     \u001b[38;5;66;03m# Fallback: use pattern matching on the string.\u001b[39;00m\n\u001b[32m   1233\u001b[39m     \u001b[38;5;66;03m# We go from longer names to shorter names to catch roberta before bert (for instance)\u001b[39;00m\n",
      "\u001b[31mValueError\u001b[39m: The checkpoint you are trying to load has model type `ernie4_5` but Transformers does not recognize this architecture. This could be because of an issue with the checkpoint, or because your version of Transformers is out of date.\n\nYou can update Transformers with the command `pip install --upgrade transformers`. If this does not work, and the checkpoint is very new, then there may not be a release version that supports this model yet. In this case, you can get the most up-to-date code by installing Transformers from source with the command `pip install git+https://github.com/huggingface/transformers.git`"
     ]
    }
   ],
   "source": [
    "# 加载 tokenizer\n",
    "print(\"加载 tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    Paths.MODEL_PATH, \n",
    "    use_fast=False, \n",
    "    trust_remote_code=True\n",
    ")\n",
    "print(\"✅ tokenizer 加载成功\")\n",
    "\n",
    "# 加载模型\n",
    "print(\"\\n加载模型...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    Paths.MODEL_PATH, \n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "print(\"✅ 模型加载成功\")\n",
    "\n",
    "# 启用梯度检查点\n",
    "model.enable_input_require_grads()\n",
    "print(\"✅ 梯度检查点已启用\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 查看模型结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 打印模型结构\n",
    "print(\"模型结构:\")\n",
    "print(model)\n",
    "\n",
    "# 查看模型参数量\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"\\n总参数量: {total_params:,}\")\n",
    "print(f\"可训练参数量: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_func(example, tokenizer):\n",
    "    \"\"\"数据预处理函数\"\"\"\n",
    "    input_ids, attention_mask, labels = [], [], []\n",
    "    \n",
    "    # 适配 chat_template\n",
    "    instruction = tokenizer(\n",
    "        f\"<|begin_of_sentence|>现在你要扮演皇帝身边的女人--甄嬛\\n\" \n",
    "        f\"User: {example['instruction']}\\n\"  \n",
    "        f\"Assistant: \",  \n",
    "        add_special_tokens=False   \n",
    "    )\n",
    "    response = tokenizer(f\"{example['output']}<|end_of_sentence|>\", add_special_tokens=False)\n",
    "    \n",
    "    # 拼接 input_ids\n",
    "    input_ids = instruction[\"input_ids\"] + response[\"input_ids\"] + [tokenizer.pad_token_id]\n",
    "    # 注意力掩码\n",
    "    attention_mask = [1] * len(input_ids)\n",
    "    # 标签，instruction 部分使用 -100 表示不计算 loss\n",
    "    labels = [-100] * len(instruction[\"input_ids\"]) + response[\"input_ids\"] + [tokenizer.pad_token_id]\n",
    "    \n",
    "    # 截断处理\n",
    "    if len(input_ids) > TrainingConfig.MAX_LENGTH:\n",
    "        input_ids = input_ids[:TrainingConfig.MAX_LENGTH]\n",
    "        attention_mask = attention_mask[:TrainingConfig.MAX_LENGTH]\n",
    "        labels = labels[:TrainingConfig.MAX_LENGTH]\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels\n",
    "    }\n",
    "\n",
    "# 数据预处理\n",
    "print(\"开始数据预处理...\")\n",
    "tokenized_dataset = dataset.map(\n",
    "    lambda example: process_func(example, tokenizer),\n",
    "    remove_columns=dataset.column_names\n",
    ")\n",
    "print(\"✅ 数据预处理完成\")\n",
    "\n",
    "# 查看预处理后的数据\n",
    "print(f\"\\n预处理后数据集大小: {len(tokenized_dataset)}\")\n",
    "print(f\"样例数据形状: {tokenized_dataset[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 配置 LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 配置 LoRA\n",
    "print(\"配置 LoRA...\")\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    target_modules=LoRAConfig.TARGET_MODULES,\n",
    "    inference_mode=False,\n",
    "    r=LoRAConfig.R,\n",
    "    lora_alpha=LoRAConfig.LORA_ALPHA,\n",
    "    lora_dropout=LoRAConfig.LORA_DROPOUT\n",
    ")\n",
    "\n",
    "print(f\"LoRA 配置:\")\n",
    "print(f\"  秩 (r): {LoRAConfig.R}\")\n",
    "print(f\"  Alpha: {LoRAConfig.LORA_ALPHA}\")\n",
    "print(f\"  Dropout: {LoRAConfig.LORA_DROPOUT}\")\n",
    "print(f\"  目标模块: {LoRAConfig.TARGET_MODULES}\")\n",
    "\n",
    "# 应用 LoRA\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "print(\"✅ LoRA 配置完成\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 配置训练参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练参数\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=Paths.OUTPUT_DIR,\n",
    "    per_device_train_batch_size=TrainingConfig.PER_DEVICE_TRAIN_BATCH_SIZE,\n",
    "    gradient_accumulation_steps=TrainingConfig.GRADIENT_ACCUMULATION_STEPS,\n",
    "    logging_steps=TrainingConfig.LOGGING_STEPS,\n",
    "    num_train_epochs=TrainingConfig.NUM_TRAIN_EPOCHS,\n",
    "    save_steps=TrainingConfig.SAVE_STEPS,\n",
    "    learning_rate=TrainingConfig.LEARNING_RATE,\n",
    "    save_on_each_node=True,\n",
    "    gradient_checkpointing=TrainingConfig.GRADIENT_CHECKPOINTING,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "print(\"训练参数:\")\n",
    "print(f\"  批量大小: {TrainingConfig.PER_DEVICE_TRAIN_BATCH_SIZE}\")\n",
    "print(f\"  梯度累积步数: {TrainingConfig.GRADIENT_ACCUMULATION_STEPS}\")\n",
    "print(f\"  学习率: {TrainingConfig.LEARNING_RATE}\")\n",
    "print(f\"  训练轮次: {TrainingConfig.NUM_TRAIN_EPOCHS}\")\n",
    "print(f\"  输出目录: {Paths.OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 配置 SwanLab（可选）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SwanLab 回调（可选）\n",
    "try:\n",
    "    swanlab_callback = SwanLabCallback(\n",
    "        project=SwanLabConfig.PROJECT_NAME, \n",
    "        experiment_name=SwanLabConfig.EXPERIMENT_NAME\n",
    "    )\n",
    "    callbacks = [swanlab_callback]\n",
    "    print(\"✅ SwanLab 回调配置成功\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠️  SwanLab 配置失败: {e}\")\n",
    "    print(\"将不使用 SwanLab 进行可视化\")\n",
    "    callbacks = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. 开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建 Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, padding=True),\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "print(\"开始训练...\")\n",
    "print(\"注意：训练过程可能需要较长时间，请耐心等待\")\n",
    "\n",
    "# 开始训练\n",
    "trainer.train()\n",
    "\n",
    "# 保存模型\n",
    "print(\"保存模型...\")\n",
    "trainer.save_model()\n",
    "\n",
    "print(\"🎉 训练完成！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. 加载训练好的模型进行推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查找最新的 checkpoint\n",
    "checkpoints = []\n",
    "if os.path.exists(Paths.OUTPUT_DIR):\n",
    "    for item in os.listdir(Paths.OUTPUT_DIR):\n",
    "        if item.startswith(\"checkpoint-\"):\n",
    "            checkpoints.append(item)\n",
    "\n",
    "if checkpoints:\n",
    "    latest_checkpoint = sorted(checkpoints, key=lambda x: int(x.split(\"-\")[1]))[-1]\n",
    "    lora_path = os.path.join(Paths.OUTPUT_DIR, latest_checkpoint)\n",
    "    print(f\"使用 checkpoint: {latest_checkpoint}\")\n",
    "else:\n",
    "    print(\"未找到训练好的模型\")\n",
    "    lora_path = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if lora_path:\n",
    "    # 重新加载基础模型\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        Paths.MODEL_PATH, \n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.bfloat16, \n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    # 加载 LoRA 权重\n",
    "    inference_model = PeftModel.from_pretrained(base_model, model_id=lora_path)\n",
    "    print(\"✅ 推理模型加载成功\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. 推理测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(model, tokenizer, prompt, system_message=None):\n",
    "    \"\"\"生成回复\"\"\"\n",
    "    if system_message is None:\n",
    "        system_message = InferenceConfig.SYSTEM_MESSAGE\n",
    "        \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    \n",
    "    # 应用 chat template\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    # 编码输入\n",
    "    model_inputs = tokenizer([text], add_special_tokens=False, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    # 生成回复\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(\n",
    "            model_inputs.input_ids,\n",
    "            max_new_tokens=InferenceConfig.MAX_NEW_TOKENS,\n",
    "            do_sample=InferenceConfig.DO_SAMPLE,\n",
    "            temperature=InferenceConfig.TEMPERATURE,\n",
    "            top_p=InferenceConfig.TOP_P,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    # 解码输出\n",
    "    output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()\n",
    "    response = tokenizer.decode(output_ids, skip_special_tokens=True).strip()\n",
    "    \n",
    "    return response\n",
    "\n",
    "# 测试对话\n",
    "if lora_path:\n",
    "    test_prompts = [\n",
    "        \"你是谁？\",\n",
    "        \"你的家人都有谁？\",\n",
    "        \"你最喜欢什么？\",\n",
    "        \"你对皇上有什么看法？\"\n",
    "    ]\n",
    "    \n",
    "    print(\"开始测试对话...\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for i, prompt in enumerate(test_prompts, 1):\n",
    "        print(f\"\\n测试 {i}: {prompt}\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "        try:\n",
    "            response = generate_response(inference_model, tokenizer, prompt)\n",
    "            print(f\"甄嬛: {response}\")\n",
    "        except Exception as e:\n",
    "            print(f\"生成回复失败: {e}\")\n",
    "        \n",
    "        print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. 交互式对话"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 交互式对话（在 Jupyter 中可能需要特殊处理）\n",
    "if lora_path:\n",
    "    print(\"交互式对话模式（输入 'quit' 退出）:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # 在 Jupyter 中，可以修改这个 cell 来测试不同的输入\n",
    "    user_input = \"你在宫中的生活如何？\"  # 修改这里来测试不同的问题\n",
    "    \n",
    "    if user_input and user_input.lower() not in ['quit', 'exit', '退出']:\n",
    "        try:\n",
    "            response = generate_response(inference_model, tokenizer, user_input)\n",
    "            print(f\"你: {user_input}\")\n",
    "            print(f\"甄嬛: {response}\")\n",
    "        except Exception as e:\n",
    "            print(f\"生成回复失败: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 总结\n",
    "\n",
    "恭喜！您已经成功完成了 ERNIE-4.5-0.3B-PT 模型的 LoRA 微调。\n",
    "\n",
    "### 主要步骤回顾：\n",
    "1. 环境配置和依赖安装\n",
    "2. 数据集加载和预处理\n",
    "3. 模型和 tokenizer 加载\n",
    "4. LoRA 配置和应用\n",
    "5. 训练参数设置\n",
    "6. 模型训练\n",
    "7. 推理测试\n",
    "\n",
    "### 下一步可以尝试：\n",
    "- 调整 LoRA 参数（r, alpha, dropout）\n",
    "- 修改训练参数（学习率、批量大小等）\n",
    "- 使用更多的数据进行训练\n",
    "- 尝试不同的目标模块组合\n",
    "\n",
    "### 相关资源：\n",
    "- [LoRA 原理详解](https://zhuanlan.zhihu.com/p/650197598)\n",
    "- [SwanLab 官网](https://swanlab.cn/)\n",
    "- [ERNIE-4.5-0.3B-PT 模型](https://www.modelscope.cn/models/PaddlePaddle/ERNIE-4.5-0.3B-PT)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
