{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ERNIE-4.5-0.3B-PT LoRA å¾®è°ƒæ•™ç¨‹\n",
    "\n",
    "æœ¬æ•™ç¨‹æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨ LoRA æŠ€æœ¯å¯¹ ERNIE-4.5-0.3B-PT æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œæ„å»ºä¸€ä¸ªèƒ½å¤Ÿæ¨¡æ‹Ÿç”„å¬›å¯¹è¯é£æ ¼çš„ä¸ªæ€§åŒ– LLMã€‚\n",
    "\n",
    "## ç¯å¢ƒè¦æ±‚\n",
    "- Python 3.12+\n",
    "- CUDA 12.4+\n",
    "- PyTorch 2.5.1+\n",
    "- æ˜¾å­˜éœ€æ±‚ï¼šçº¦ 24GB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. å¯¼å…¥å¿…è¦çš„åº“"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… åº“å¯¼å…¥æˆåŠŸ\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM, \n",
    "    TrainingArguments, \n",
    "    Trainer, \n",
    "    DataCollatorForSeq2Seq\n",
    ")\n",
    "from peft import LoraConfig, TaskType, get_peft_model, PeftModel\n",
    "import swanlab\n",
    "from swanlab.integration.transformers import SwanLabCallback\n",
    "from config import Paths, LoRAConfig, TrainingConfig, SwanLabConfig, InferenceConfig\n",
    "\n",
    "print(\"âœ… åº“å¯¼å…¥æˆåŠŸ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. æ£€æŸ¥ç¯å¢ƒå’Œè·¯å¾„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ¨¡å‹è·¯å¾„: d:\\Sources\\self-llm\\examples\\ERNIE-4.5-0.3B-PT-Lora\\models\\PaddlePaddle\\ERNIE-4___5-0___3B-PT\n",
      "æ¨¡å‹å­˜åœ¨: True\n",
      "æ•°æ®é›†è·¯å¾„: d:\\Sources\\self-llm\\examples\\ERNIE-4.5-0.3B-PT-Lora\\..\\..\\dataset\\huanhuan.json\n",
      "æ•°æ®é›†å­˜åœ¨: True\n",
      "CUDA å¯ç”¨: True\n",
      "GPU æ•°é‡: 1\n",
      "å½“å‰ GPU: NVIDIA GeForce RTX 2080 Ti\n"
     ]
    }
   ],
   "source": [
    "# æ£€æŸ¥æ¨¡å‹è·¯å¾„\n",
    "print(f\"æ¨¡å‹è·¯å¾„: {Paths.MODEL_PATH}\")\n",
    "print(f\"æ¨¡å‹å­˜åœ¨: {os.path.exists(Paths.MODEL_PATH)}\")\n",
    "\n",
    "# æ£€æŸ¥æ•°æ®é›†è·¯å¾„\n",
    "print(f\"æ•°æ®é›†è·¯å¾„: {Paths.DATASET_PATH}\")\n",
    "print(f\"æ•°æ®é›†å­˜åœ¨: {os.path.exists(Paths.DATASET_PATH)}\")\n",
    "\n",
    "# æ£€æŸ¥ CUDA\n",
    "print(f\"CUDA å¯ç”¨: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU æ•°é‡: {torch.cuda.device_count()}\")\n",
    "    print(f\"å½“å‰ GPU: {torch.cuda.get_device_name()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. åŠ è½½æ•°æ®é›†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "åŠ è½½æ•°æ®é›†: d:\\Sources\\self-llm\\examples\\ERNIE-4.5-0.3B-PT-Lora\\..\\..\\dataset\\huanhuan.json\n",
      "æ•°æ®é›†å¤§å°: 3729\n",
      "\n",
      "æ•°æ®é›†æ ·ä¾‹:\n",
      "æ ·ä¾‹ 1:\n",
      "  æŒ‡ä»¤: å°å§ï¼Œåˆ«çš„ç§€å¥³éƒ½åœ¨æ±‚ä¸­é€‰ï¼Œå”¯æœ‰å’±ä»¬å°å§æƒ³è¢«æ’‚ç‰Œå­ï¼Œè©è¨ä¸€å®šè®°å¾—çœŸçœŸå„¿çš„â€”â€”\n",
      "  è¾“å‡º: å˜˜â€”â€”éƒ½è¯´è®¸æ„¿è¯´ç ´æ˜¯ä¸çµçš„ã€‚\n",
      "\n",
      "æ ·ä¾‹ 2:\n",
      "  æŒ‡ä»¤: è¿™ä¸ªæ¸©å¤ªåŒ»å•Šï¼Œä¹Ÿæ˜¯å¤æ€ªï¼Œè°ä¸çŸ¥å¤ªåŒ»ä¸å¾—çš‡å‘½ä¸èƒ½ä¸ºçš‡æ—ä»¥å¤–çš„äººè¯·è„‰è¯Šç—…ï¼Œä»–å€’å¥½ï¼Œåå¤©åŠæœˆä¾¿å¾€å’±ä»¬åºœé‡Œè·‘ã€‚\n",
      "  è¾“å‡º: ä½ ä»¬ä¿©è¯å¤ªå¤šäº†ï¼Œæˆ‘è¯¥å’Œæ¸©å¤ªåŒ»è¦ä¸€å‰‚è¯ï¼Œå¥½å¥½æ²»æ²»ä½ ä»¬ã€‚\n",
      "\n",
      "æ ·ä¾‹ 3:\n",
      "  æŒ‡ä»¤: å¬›å¦¹å¦¹ï¼Œåˆšåˆšæˆ‘å»åºœä¸Šè¯·è„‰ï¼Œå¬ç”„ä¼¯æ¯è¯´ä½ æ¥è¿™é‡Œè¿›é¦™äº†ã€‚\n",
      "  è¾“å‡º: å‡ºæ¥èµ°èµ°ï¼Œä¹Ÿæ˜¯æ•£å¿ƒã€‚\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def load_dataset(data_path):\n",
    "    \"\"\"åŠ è½½ç”„å¬›æ•°æ®é›†\"\"\"\n",
    "    print(f\"åŠ è½½æ•°æ®é›†: {data_path}\")\n",
    "    with open(data_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    print(f\"æ•°æ®é›†å¤§å°: {len(data)}\")\n",
    "    return Dataset.from_list(data)\n",
    "\n",
    "# åŠ è½½æ•°æ®é›†\n",
    "dataset = load_dataset(Paths.DATASET_PATH)\n",
    "\n",
    "# æŸ¥çœ‹æ•°æ®é›†æ ·ä¾‹\n",
    "print(\"\\næ•°æ®é›†æ ·ä¾‹:\")\n",
    "for i in range(min(3, len(dataset))):\n",
    "    print(f\"æ ·ä¾‹ {i+1}:\")\n",
    "    print(f\"  æŒ‡ä»¤: {dataset[i]['instruction']}\")\n",
    "    print(f\"  è¾“å‡º: {dataset[i]['output']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. åŠ è½½æ¨¡å‹å’Œ Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "åŠ è½½ tokenizer...\n",
      "âœ… tokenizer åŠ è½½æˆåŠŸ\n",
      "\n",
      "åŠ è½½æ¨¡å‹...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The checkpoint you are trying to load has model type `ernie4_5` but Transformers does not recognize this architecture. This could be because of an issue with the checkpoint, or because your version of Transformers is out of date.\n\nYou can update Transformers with the command `pip install --upgrade transformers`. If this does not work, and the checkpoint is very new, then there may not be a release version that supports this model yet. In this case, you can get the most up-to-date code by installing Transformers from source with the command `pip install git+https://github.com/huggingface/transformers.git`",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Sources\\self-llm\\examples\\ERNIE-4.5-0.3B-PT-Lora\\.venv\\Lib\\site-packages\\transformers\\models\\auto\\configuration_auto.py:1218\u001b[39m, in \u001b[36mAutoConfig.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[39m\n\u001b[32m   1217\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1218\u001b[39m     config_class = \u001b[43mCONFIG_MAPPING\u001b[49m\u001b[43m[\u001b[49m\u001b[43mconfig_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel_type\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m   1219\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Sources\\self-llm\\examples\\ERNIE-4.5-0.3B-PT-Lora\\.venv\\Lib\\site-packages\\transformers\\models\\auto\\configuration_auto.py:914\u001b[39m, in \u001b[36m_LazyConfigMapping.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m    913\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._mapping:\n\u001b[32m--> \u001b[39m\u001b[32m914\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n\u001b[32m    915\u001b[39m value = \u001b[38;5;28mself\u001b[39m._mapping[key]\n",
      "\u001b[31mKeyError\u001b[39m: 'ernie4_5'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# åŠ è½½æ¨¡å‹\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33måŠ è½½æ¨¡å‹...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m model = \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mPaths\u001b[49m\u001b[43m.\u001b[49m\u001b[43mMODEL_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mauto\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbfloat16\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m     17\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mâœ… æ¨¡å‹åŠ è½½æˆåŠŸ\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Sources\\self-llm\\examples\\ERNIE-4.5-0.3B-PT-Lora\\.venv\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:547\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    544\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mquantization_config\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    545\u001b[39m     _ = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mquantization_config\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m547\u001b[39m config, kwargs = \u001b[43mAutoConfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    548\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    549\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_unused_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    550\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcode_revision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcode_revision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    551\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    552\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    553\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    554\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    556\u001b[39m \u001b[38;5;66;03m# if torch_dtype=auto was passed here, ensure to pass it on\u001b[39;00m\n\u001b[32m    557\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m kwargs_orig.get(\u001b[33m\"\u001b[39m\u001b[33mtorch_dtype\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) == \u001b[33m\"\u001b[39m\u001b[33mauto\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Sources\\self-llm\\examples\\ERNIE-4.5-0.3B-PT-Lora\\.venv\\Lib\\site-packages\\transformers\\models\\auto\\configuration_auto.py:1220\u001b[39m, in \u001b[36mAutoConfig.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[39m\n\u001b[32m   1218\u001b[39m         config_class = CONFIG_MAPPING[config_dict[\u001b[33m\"\u001b[39m\u001b[33mmodel_type\u001b[39m\u001b[33m\"\u001b[39m]]\n\u001b[32m   1219\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1220\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1221\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe checkpoint you are trying to load has model type `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig_dict[\u001b[33m'\u001b[39m\u001b[33mmodel_type\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m` \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1222\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mbut Transformers does not recognize this architecture. This could be because of an \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1223\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33missue with the checkpoint, or because your version of Transformers is out of date.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1224\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mYou can update Transformers with the command `pip install --upgrade transformers`. If this \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1225\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mdoes not work, and the checkpoint is very new, then there may not be a release version \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1226\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mthat supports this model yet. In this case, you can get the most up-to-date code by installing \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1227\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mTransformers from source with the command \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1228\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m`pip install git+https://github.com/huggingface/transformers.git`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1229\u001b[39m         )\n\u001b[32m   1230\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m config_class.from_dict(config_dict, **unused_kwargs)\n\u001b[32m   1231\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1232\u001b[39m     \u001b[38;5;66;03m# Fallback: use pattern matching on the string.\u001b[39;00m\n\u001b[32m   1233\u001b[39m     \u001b[38;5;66;03m# We go from longer names to shorter names to catch roberta before bert (for instance)\u001b[39;00m\n",
      "\u001b[31mValueError\u001b[39m: The checkpoint you are trying to load has model type `ernie4_5` but Transformers does not recognize this architecture. This could be because of an issue with the checkpoint, or because your version of Transformers is out of date.\n\nYou can update Transformers with the command `pip install --upgrade transformers`. If this does not work, and the checkpoint is very new, then there may not be a release version that supports this model yet. In this case, you can get the most up-to-date code by installing Transformers from source with the command `pip install git+https://github.com/huggingface/transformers.git`"
     ]
    }
   ],
   "source": [
    "# åŠ è½½ tokenizer\n",
    "print(\"åŠ è½½ tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    Paths.MODEL_PATH, \n",
    "    use_fast=False, \n",
    "    trust_remote_code=True\n",
    ")\n",
    "print(\"âœ… tokenizer åŠ è½½æˆåŠŸ\")\n",
    "\n",
    "# åŠ è½½æ¨¡å‹\n",
    "print(\"\\nåŠ è½½æ¨¡å‹...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    Paths.MODEL_PATH, \n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "print(\"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ\")\n",
    "\n",
    "# å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹\n",
    "model.enable_input_require_grads()\n",
    "print(\"âœ… æ¢¯åº¦æ£€æŸ¥ç‚¹å·²å¯ç”¨\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. æŸ¥çœ‹æ¨¡å‹ç»“æ„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ‰“å°æ¨¡å‹ç»“æ„\n",
    "print(\"æ¨¡å‹ç»“æ„:\")\n",
    "print(model)\n",
    "\n",
    "# æŸ¥çœ‹æ¨¡å‹å‚æ•°é‡\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"\\næ€»å‚æ•°é‡: {total_params:,}\")\n",
    "print(f\"å¯è®­ç»ƒå‚æ•°é‡: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. æ•°æ®é¢„å¤„ç†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_func(example, tokenizer):\n",
    "    \"\"\"æ•°æ®é¢„å¤„ç†å‡½æ•°\"\"\"\n",
    "    input_ids, attention_mask, labels = [], [], []\n",
    "    \n",
    "    # é€‚é… chat_template\n",
    "    instruction = tokenizer(\n",
    "        f\"<|begin_of_sentence|>ç°åœ¨ä½ è¦æ‰®æ¼”çš‡å¸èº«è¾¹çš„å¥³äºº--ç”„å¬›\\n\" \n",
    "        f\"User: {example['instruction']}\\n\"  \n",
    "        f\"Assistant: \",  \n",
    "        add_special_tokens=False   \n",
    "    )\n",
    "    response = tokenizer(f\"{example['output']}<|end_of_sentence|>\", add_special_tokens=False)\n",
    "    \n",
    "    # æ‹¼æ¥ input_ids\n",
    "    input_ids = instruction[\"input_ids\"] + response[\"input_ids\"] + [tokenizer.pad_token_id]\n",
    "    # æ³¨æ„åŠ›æ©ç \n",
    "    attention_mask = [1] * len(input_ids)\n",
    "    # æ ‡ç­¾ï¼Œinstruction éƒ¨åˆ†ä½¿ç”¨ -100 è¡¨ç¤ºä¸è®¡ç®— loss\n",
    "    labels = [-100] * len(instruction[\"input_ids\"]) + response[\"input_ids\"] + [tokenizer.pad_token_id]\n",
    "    \n",
    "    # æˆªæ–­å¤„ç†\n",
    "    if len(input_ids) > TrainingConfig.MAX_LENGTH:\n",
    "        input_ids = input_ids[:TrainingConfig.MAX_LENGTH]\n",
    "        attention_mask = attention_mask[:TrainingConfig.MAX_LENGTH]\n",
    "        labels = labels[:TrainingConfig.MAX_LENGTH]\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels\n",
    "    }\n",
    "\n",
    "# æ•°æ®é¢„å¤„ç†\n",
    "print(\"å¼€å§‹æ•°æ®é¢„å¤„ç†...\")\n",
    "tokenized_dataset = dataset.map(\n",
    "    lambda example: process_func(example, tokenizer),\n",
    "    remove_columns=dataset.column_names\n",
    ")\n",
    "print(\"âœ… æ•°æ®é¢„å¤„ç†å®Œæˆ\")\n",
    "\n",
    "# æŸ¥çœ‹é¢„å¤„ç†åçš„æ•°æ®\n",
    "print(f\"\\né¢„å¤„ç†åæ•°æ®é›†å¤§å°: {len(tokenized_dataset)}\")\n",
    "print(f\"æ ·ä¾‹æ•°æ®å½¢çŠ¶: {tokenized_dataset[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. é…ç½® LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# é…ç½® LoRA\n",
    "print(\"é…ç½® LoRA...\")\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    target_modules=LoRAConfig.TARGET_MODULES,\n",
    "    inference_mode=False,\n",
    "    r=LoRAConfig.R,\n",
    "    lora_alpha=LoRAConfig.LORA_ALPHA,\n",
    "    lora_dropout=LoRAConfig.LORA_DROPOUT\n",
    ")\n",
    "\n",
    "print(f\"LoRA é…ç½®:\")\n",
    "print(f\"  ç§© (r): {LoRAConfig.R}\")\n",
    "print(f\"  Alpha: {LoRAConfig.LORA_ALPHA}\")\n",
    "print(f\"  Dropout: {LoRAConfig.LORA_DROPOUT}\")\n",
    "print(f\"  ç›®æ ‡æ¨¡å—: {LoRAConfig.TARGET_MODULES}\")\n",
    "\n",
    "# åº”ç”¨ LoRA\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "print(\"âœ… LoRA é…ç½®å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. é…ç½®è®­ç»ƒå‚æ•°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è®­ç»ƒå‚æ•°\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=Paths.OUTPUT_DIR,\n",
    "    per_device_train_batch_size=TrainingConfig.PER_DEVICE_TRAIN_BATCH_SIZE,\n",
    "    gradient_accumulation_steps=TrainingConfig.GRADIENT_ACCUMULATION_STEPS,\n",
    "    logging_steps=TrainingConfig.LOGGING_STEPS,\n",
    "    num_train_epochs=TrainingConfig.NUM_TRAIN_EPOCHS,\n",
    "    save_steps=TrainingConfig.SAVE_STEPS,\n",
    "    learning_rate=TrainingConfig.LEARNING_RATE,\n",
    "    save_on_each_node=True,\n",
    "    gradient_checkpointing=TrainingConfig.GRADIENT_CHECKPOINTING,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "print(\"è®­ç»ƒå‚æ•°:\")\n",
    "print(f\"  æ‰¹é‡å¤§å°: {TrainingConfig.PER_DEVICE_TRAIN_BATCH_SIZE}\")\n",
    "print(f\"  æ¢¯åº¦ç´¯ç§¯æ­¥æ•°: {TrainingConfig.GRADIENT_ACCUMULATION_STEPS}\")\n",
    "print(f\"  å­¦ä¹ ç‡: {TrainingConfig.LEARNING_RATE}\")\n",
    "print(f\"  è®­ç»ƒè½®æ¬¡: {TrainingConfig.NUM_TRAIN_EPOCHS}\")\n",
    "print(f\"  è¾“å‡ºç›®å½•: {Paths.OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. é…ç½® SwanLabï¼ˆå¯é€‰ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SwanLab å›è°ƒï¼ˆå¯é€‰ï¼‰\n",
    "try:\n",
    "    swanlab_callback = SwanLabCallback(\n",
    "        project=SwanLabConfig.PROJECT_NAME, \n",
    "        experiment_name=SwanLabConfig.EXPERIMENT_NAME\n",
    "    )\n",
    "    callbacks = [swanlab_callback]\n",
    "    print(\"âœ… SwanLab å›è°ƒé…ç½®æˆåŠŸ\")\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸  SwanLab é…ç½®å¤±è´¥: {e}\")\n",
    "    print(\"å°†ä¸ä½¿ç”¨ SwanLab è¿›è¡Œå¯è§†åŒ–\")\n",
    "    callbacks = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. å¼€å§‹è®­ç»ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ›å»º Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, padding=True),\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "print(\"å¼€å§‹è®­ç»ƒ...\")\n",
    "print(\"æ³¨æ„ï¼šè®­ç»ƒè¿‡ç¨‹å¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´ï¼Œè¯·è€å¿ƒç­‰å¾…\")\n",
    "\n",
    "# å¼€å§‹è®­ç»ƒ\n",
    "trainer.train()\n",
    "\n",
    "# ä¿å­˜æ¨¡å‹\n",
    "print(\"ä¿å­˜æ¨¡å‹...\")\n",
    "trainer.save_model()\n",
    "\n",
    "print(\"ğŸ‰ è®­ç»ƒå®Œæˆï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹è¿›è¡Œæ¨ç†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æŸ¥æ‰¾æœ€æ–°çš„ checkpoint\n",
    "checkpoints = []\n",
    "if os.path.exists(Paths.OUTPUT_DIR):\n",
    "    for item in os.listdir(Paths.OUTPUT_DIR):\n",
    "        if item.startswith(\"checkpoint-\"):\n",
    "            checkpoints.append(item)\n",
    "\n",
    "if checkpoints:\n",
    "    latest_checkpoint = sorted(checkpoints, key=lambda x: int(x.split(\"-\")[1]))[-1]\n",
    "    lora_path = os.path.join(Paths.OUTPUT_DIR, latest_checkpoint)\n",
    "    print(f\"ä½¿ç”¨ checkpoint: {latest_checkpoint}\")\n",
    "else:\n",
    "    print(\"æœªæ‰¾åˆ°è®­ç»ƒå¥½çš„æ¨¡å‹\")\n",
    "    lora_path = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if lora_path:\n",
    "    # é‡æ–°åŠ è½½åŸºç¡€æ¨¡å‹\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        Paths.MODEL_PATH, \n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.bfloat16, \n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    # åŠ è½½ LoRA æƒé‡\n",
    "    inference_model = PeftModel.from_pretrained(base_model, model_id=lora_path)\n",
    "    print(\"âœ… æ¨ç†æ¨¡å‹åŠ è½½æˆåŠŸ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. æ¨ç†æµ‹è¯•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(model, tokenizer, prompt, system_message=None):\n",
    "    \"\"\"ç”Ÿæˆå›å¤\"\"\"\n",
    "    if system_message is None:\n",
    "        system_message = InferenceConfig.SYSTEM_MESSAGE\n",
    "        \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    \n",
    "    # åº”ç”¨ chat template\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    # ç¼–ç è¾“å…¥\n",
    "    model_inputs = tokenizer([text], add_special_tokens=False, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    # ç”Ÿæˆå›å¤\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(\n",
    "            model_inputs.input_ids,\n",
    "            max_new_tokens=InferenceConfig.MAX_NEW_TOKENS,\n",
    "            do_sample=InferenceConfig.DO_SAMPLE,\n",
    "            temperature=InferenceConfig.TEMPERATURE,\n",
    "            top_p=InferenceConfig.TOP_P,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    # è§£ç è¾“å‡º\n",
    "    output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()\n",
    "    response = tokenizer.decode(output_ids, skip_special_tokens=True).strip()\n",
    "    \n",
    "    return response\n",
    "\n",
    "# æµ‹è¯•å¯¹è¯\n",
    "if lora_path:\n",
    "    test_prompts = [\n",
    "        \"ä½ æ˜¯è°ï¼Ÿ\",\n",
    "        \"ä½ çš„å®¶äººéƒ½æœ‰è°ï¼Ÿ\",\n",
    "        \"ä½ æœ€å–œæ¬¢ä»€ä¹ˆï¼Ÿ\",\n",
    "        \"ä½ å¯¹çš‡ä¸Šæœ‰ä»€ä¹ˆçœ‹æ³•ï¼Ÿ\"\n",
    "    ]\n",
    "    \n",
    "    print(\"å¼€å§‹æµ‹è¯•å¯¹è¯...\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for i, prompt in enumerate(test_prompts, 1):\n",
    "        print(f\"\\næµ‹è¯• {i}: {prompt}\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "        try:\n",
    "            response = generate_response(inference_model, tokenizer, prompt)\n",
    "            print(f\"ç”„å¬›: {response}\")\n",
    "        except Exception as e:\n",
    "            print(f\"ç”Ÿæˆå›å¤å¤±è´¥: {e}\")\n",
    "        \n",
    "        print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. äº¤äº’å¼å¯¹è¯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# äº¤äº’å¼å¯¹è¯ï¼ˆåœ¨ Jupyter ä¸­å¯èƒ½éœ€è¦ç‰¹æ®Šå¤„ç†ï¼‰\n",
    "if lora_path:\n",
    "    print(\"äº¤äº’å¼å¯¹è¯æ¨¡å¼ï¼ˆè¾“å…¥ 'quit' é€€å‡ºï¼‰:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # åœ¨ Jupyter ä¸­ï¼Œå¯ä»¥ä¿®æ”¹è¿™ä¸ª cell æ¥æµ‹è¯•ä¸åŒçš„è¾“å…¥\n",
    "    user_input = \"ä½ åœ¨å®«ä¸­çš„ç”Ÿæ´»å¦‚ä½•ï¼Ÿ\"  # ä¿®æ”¹è¿™é‡Œæ¥æµ‹è¯•ä¸åŒçš„é—®é¢˜\n",
    "    \n",
    "    if user_input and user_input.lower() not in ['quit', 'exit', 'é€€å‡º']:\n",
    "        try:\n",
    "            response = generate_response(inference_model, tokenizer, user_input)\n",
    "            print(f\"ä½ : {user_input}\")\n",
    "            print(f\"ç”„å¬›: {response}\")\n",
    "        except Exception as e:\n",
    "            print(f\"ç”Ÿæˆå›å¤å¤±è´¥: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ€»ç»“\n",
    "\n",
    "æ­å–œï¼æ‚¨å·²ç»æˆåŠŸå®Œæˆäº† ERNIE-4.5-0.3B-PT æ¨¡å‹çš„ LoRA å¾®è°ƒã€‚\n",
    "\n",
    "### ä¸»è¦æ­¥éª¤å›é¡¾ï¼š\n",
    "1. ç¯å¢ƒé…ç½®å’Œä¾èµ–å®‰è£…\n",
    "2. æ•°æ®é›†åŠ è½½å’Œé¢„å¤„ç†\n",
    "3. æ¨¡å‹å’Œ tokenizer åŠ è½½\n",
    "4. LoRA é…ç½®å’Œåº”ç”¨\n",
    "5. è®­ç»ƒå‚æ•°è®¾ç½®\n",
    "6. æ¨¡å‹è®­ç»ƒ\n",
    "7. æ¨ç†æµ‹è¯•\n",
    "\n",
    "### ä¸‹ä¸€æ­¥å¯ä»¥å°è¯•ï¼š\n",
    "- è°ƒæ•´ LoRA å‚æ•°ï¼ˆr, alpha, dropoutï¼‰\n",
    "- ä¿®æ”¹è®­ç»ƒå‚æ•°ï¼ˆå­¦ä¹ ç‡ã€æ‰¹é‡å¤§å°ç­‰ï¼‰\n",
    "- ä½¿ç”¨æ›´å¤šçš„æ•°æ®è¿›è¡Œè®­ç»ƒ\n",
    "- å°è¯•ä¸åŒçš„ç›®æ ‡æ¨¡å—ç»„åˆ\n",
    "\n",
    "### ç›¸å…³èµ„æºï¼š\n",
    "- [LoRA åŸç†è¯¦è§£](https://zhuanlan.zhihu.com/p/650197598)\n",
    "- [SwanLab å®˜ç½‘](https://swanlab.cn/)\n",
    "- [ERNIE-4.5-0.3B-PT æ¨¡å‹](https://www.modelscope.cn/models/PaddlePaddle/ERNIE-4.5-0.3B-PT)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
