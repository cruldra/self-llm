"""
Qwen3-8B vLLM API æœåŠ¡å™¨å¯åŠ¨è„šæœ¬

ä½¿ç”¨ vLLM å¯åŠ¨å…¼å®¹ OpenAI API çš„æœåŠ¡å™¨
"""

import os
import sys
import subprocess
import argparse
from pathlib import Path


def get_model_path() -> str:
    """è·å–æ¨¡å‹è·¯å¾„"""
    # ä¼˜å…ˆæ£€æŸ¥ AutoDL ç¯å¢ƒ
    autodl_path = "/root/autodl-tmp/Qwen/Qwen3-8B"
    if os.path.exists(autodl_path):
        return autodl_path
    
    # æ£€æŸ¥æœ¬åœ° models ç›®å½•
    local_path = "./models/Qwen/Qwen3-8B"
    if os.path.exists(local_path):
        return local_path
    
    # å¦‚æœéƒ½ä¸å­˜åœ¨ï¼Œè¿”å›é»˜è®¤è·¯å¾„å¹¶æç¤ºç”¨æˆ·
    print("âš ï¸  æœªæ‰¾åˆ°æ¨¡å‹æ–‡ä»¶ï¼Œè¯·å…ˆè¿è¡Œ model_download.py ä¸‹è½½æ¨¡å‹")
    return local_path


def check_model_exists(model_path: str) -> bool:
    """æ£€æŸ¥æ¨¡å‹æ˜¯å¦å­˜åœ¨"""
    if not os.path.exists(model_path):
        print(f"âŒ æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}")
        print("è¯·å…ˆè¿è¡Œä»¥ä¸‹å‘½ä»¤ä¸‹è½½æ¨¡å‹:")
        print("   uv run python model_download.py")
        return False
    
    # æ£€æŸ¥å…³é”®æ–‡ä»¶
    required_files = ["config.json", "tokenizer.json"]
    for file_name in required_files:
        if not os.path.exists(os.path.join(model_path, file_name)):
            print(f"âŒ ç¼ºå°‘æ¨¡å‹æ–‡ä»¶: {file_name}")
            return False
    
    return True


def start_vllm_server(
    model_path: str,
    host: str = "0.0.0.0",
    port: int = 8000,
    served_model_name: str = "Qwen3-8B",
    max_model_len: int = 8192,
    enable_reasoning: bool = True,
    reasoning_parser: str = "qwen3",
    gpu_memory_utilization: float = 0.9,
    tensor_parallel_size: int = 1
):
    """
    å¯åŠ¨ vLLM API æœåŠ¡å™¨
    
    Args:
        model_path: æ¨¡å‹è·¯å¾„
        host: æœåŠ¡å™¨ä¸»æœºåœ°å€
        port: æœåŠ¡å™¨ç«¯å£
        served_model_name: æœåŠ¡æ¨¡å‹åç§°
        max_model_len: æ¨¡å‹æœ€å¤§é•¿åº¦
        enable_reasoning: æ˜¯å¦å¯ç”¨æ¨ç†æ¨¡å¼
        reasoning_parser: æ¨ç†è§£æå™¨
        gpu_memory_utilization: GPU å†…å­˜åˆ©ç”¨ç‡
        tensor_parallel_size: å¼ é‡å¹¶è¡Œå¤§å°
    """
    # è®¾ç½®ç¯å¢ƒå˜é‡
    env = os.environ.copy()
    env['VLLM_USE_MODELSCOPE'] = 'true'
    
    # æ„å»ºå‘½ä»¤
    cmd = [
        "vllm", "serve", model_path,
        "--host", host,
        "--port", str(port),
        "--served-model-name", served_model_name,
        "--max-model-len", str(max_model_len),
        "--gpu-memory-utilization", str(gpu_memory_utilization),
        "--tensor-parallel-size", str(tensor_parallel_size),
        "--trust-remote-code"
    ]
    
    # æ·»åŠ æ¨ç†æ¨¡å¼å‚æ•°
    if enable_reasoning:
        cmd.extend(["--reasoning-parser", reasoning_parser])
    
    print("ğŸš€ å¯åŠ¨ vLLM API æœåŠ¡å™¨")
    print("=" * 50)
    print(f"ğŸ“ æ¨¡å‹è·¯å¾„: {model_path}")
    print(f"ğŸŒ æœåŠ¡åœ°å€: http://{host}:{port}")
    print(f"ğŸ·ï¸  æ¨¡å‹åç§°: {served_model_name}")
    print(f"ğŸ“ æœ€å¤§é•¿åº¦: {max_model_len}")
    print(f"ğŸ§  æ¨ç†æ¨¡å¼: {'å¯ç”¨' if enable_reasoning else 'ç¦ç”¨'}")
    print(f"ğŸ¯ GPU åˆ©ç”¨ç‡: {gpu_memory_utilization}")
    print(f"âš¡ å¹¶è¡Œå¤§å°: {tensor_parallel_size}")
    print("=" * 50)
    
    print("æ‰§è¡Œå‘½ä»¤:")
    print(" ".join(cmd))
    print("\nâ³ æ­£åœ¨å¯åŠ¨æœåŠ¡å™¨...")
    print("ğŸ’¡ æç¤º: æœåŠ¡å™¨å¯åŠ¨åï¼Œå¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼æµ‹è¯•:")
    print(f"   curl http://localhost:{port}/v1/models")
    print(f"   uv run python test_openai_chat_completions.py")
    print("\næŒ‰ Ctrl+C åœæ­¢æœåŠ¡å™¨")
    print("-" * 50)
    
    try:
        # å¯åŠ¨æœåŠ¡å™¨
        subprocess.run(cmd, env=env, check=True)
    except KeyboardInterrupt:
        print("\nğŸ‘‹ æœåŠ¡å™¨å·²åœæ­¢")
    except subprocess.CalledProcessError as e:
        print(f"âŒ æœåŠ¡å™¨å¯åŠ¨å¤±è´¥: {e}")
        sys.exit(1)


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="å¯åŠ¨ Qwen3-8B vLLM API æœåŠ¡å™¨")
    
    parser.add_argument(
        "--model-path",
        type=str,
        default=None,
        help="æ¨¡å‹è·¯å¾„ (é»˜è®¤è‡ªåŠ¨æ£€æµ‹)"
    )
    parser.add_argument(
        "--host",
        type=str,
        default="0.0.0.0",
        help="æœåŠ¡å™¨ä¸»æœºåœ°å€ (é»˜è®¤: 0.0.0.0)"
    )
    parser.add_argument(
        "--port",
        type=int,
        default=8000,
        help="æœåŠ¡å™¨ç«¯å£ (é»˜è®¤: 8000)"
    )
    parser.add_argument(
        "--served-model-name",
        type=str,
        default="Qwen3-8B",
        help="æœåŠ¡æ¨¡å‹åç§° (é»˜è®¤: Qwen3-8B)"
    )
    parser.add_argument(
        "--max-model-len",
        type=int,
        default=8192,
        help="æ¨¡å‹æœ€å¤§é•¿åº¦ (é»˜è®¤: 8192)"
    )
    parser.add_argument(
        "--disable-reasoning",
        action="store_true",
        help="ç¦ç”¨æ¨ç†æ¨¡å¼"
    )
    parser.add_argument(
        "--reasoning-parser",
        type=str,
        default="qwen3",
        help="æ¨ç†è§£æå™¨ (é»˜è®¤: qwen3)"
    )
    parser.add_argument(
        "--gpu-memory-utilization",
        type=float,
        default=0.9,
        help="GPU å†…å­˜åˆ©ç”¨ç‡ (é»˜è®¤: 0.9)"
    )
    parser.add_argument(
        "--tensor-parallel-size",
        type=int,
        default=1,
        help="å¼ é‡å¹¶è¡Œå¤§å° (é»˜è®¤: 1)"
    )
    
    args = parser.parse_args()
    
    # è·å–æ¨¡å‹è·¯å¾„
    model_path = args.model_path or get_model_path()
    
    # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å­˜åœ¨
    if not check_model_exists(model_path):
        sys.exit(1)
    
    # å¯åŠ¨æœåŠ¡å™¨
    start_vllm_server(
        model_path=model_path,
        host=args.host,
        port=args.port,
        served_model_name=args.served_model_name,
        max_model_len=args.max_model_len,
        enable_reasoning=not args.disable_reasoning,
        reasoning_parser=args.reasoning_parser,
        gpu_memory_utilization=args.gpu_memory_utilization,
        tensor_parallel_size=args.tensor_parallel_size
    )


if __name__ == "__main__":
    main()
